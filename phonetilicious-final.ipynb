{
 "metadata": {
  "name": "",
  "signature": "sha256:1816b71fcc5616a97a6a16abeb8d3d2f14eeb48c86932e865b725591b26454e3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from topia.termextract import extract\n",
      "extractor = extract.TermExtractor()\t\n",
      "import pattern.en\n",
      "\n",
      "using_topia_tagger = True\n",
      "\n",
      "def extract_words(sentence):\n",
      "    # tokenized sentence. returns a list of [word, part of speech, word_singular]\n",
      "    if(using_topia_tagger):\n",
      "        tok = extractor.tagger(sentence)\n",
      "        if not tok:\n",
      "            return []\n",
      "        # hack for fixing the bug which sometimes makes extractor's first word of the senten lower case\n",
      "        first = sentence.strip()[:1] # first letter\n",
      "        if (first.upper() == first):\n",
      "            tok[0][0] = tok[0][0][:1].upper() + tok[0][0][1:]\n",
      "        else:\n",
      "            tok[0][0] = tok[0][0][:1] + tok[0][0][1:]\n",
      "    else:\n",
      "        # use nltk's tagger maxent_treebank_pos_tagger\n",
      "        text = word_tokenize(sentence)\n",
      "        tok = pos_tag(text)\n",
      "    \"\"\"\n",
      "    # pattern's tagger:\n",
      "    pattern.en.tag(text)\n",
      "    \"\"\"\n",
      "        \n",
      "    #if DEBUG:\n",
      "        #print tok \n",
      "        \n",
      "    # nothing to extract\n",
      "    if len(tok) == 0:\n",
      "        return []\n",
      "    \n",
      "    \n",
      "    formatted = map(lambda w: {\"word_original\": w[0], \"word_new\": w[0], \"ignore\": False, \"pos\": w[1], \"word_lower\": justlowerletters(w[0])}, tok)\n",
      "    \n",
      "\n",
      "    return formatted\n",
      "    #sentence = justlowerletters(sentence)\n",
      "    #return sentence.split(\" \")\n",
      "    \n",
      "# given string, returns ascii string, lowercase\n",
      "def justlowerletters(string):\n",
      "    string = string.encode('ascii','ignore')\n",
      "    #string = str(string)\n",
      "    return str.lower(string)\n",
      "    #return sub(r'\\s+',' ',sub(r'[^a-z\\s]', '', str.lower(string)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given sentence, returns list of formatted words\n",
      "# TODO: consider using a tokenizer\n",
      "from nltk.corpus import stopwords \n",
      "stop = map(lambda string: string.encode('ascii','ignore'), stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.corpus import cmudict as cmu\n",
      "from re import sub, split\n",
      "from random import randrange\n",
      "from numpy import *\n",
      "from nltk import word_tokenize, sent_tokenize, pos_tag    \n",
      "from random import shuffle\n",
      "import re\n",
      "\n",
      "DEBUG = True\n",
      "\n",
      "prondict = cmu.dict() \n",
      "Phonemes = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\", \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
      "Consonants = [\"B\", \"CH\", \"D\", \"DH\", \"F\", \"G\", \"HH\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
      "Vowels =  [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"EH\", \"ER\", \"EY\", \"IH\", \"IY\", \"OW\", \"OY\", \"UH\", \"UW\"]\n",
      "\n",
      "num_phonemes = len(Phonemes)\n",
      "max_synonyms = 100\n",
      "\n",
      "# TODO\n",
      "# consonance_only\n",
      "# assonance_only\n",
      "# look at first phoneme of each word\n",
      "# look at first phoneme of each syllable\n",
      "# look at first phoneme of stressed syllables\n",
      "# swap out synonyms in same part of speech only\n",
      "# only swap out Nouns, Adjectives, Verbs, Adverbs\n",
      "# ignore most common words\n",
      "# keep punctuation\n",
      "\n",
      "#alliteration\n",
      "\n",
      "# HEURISTIC. Efficient. Runs in n*m time. \n",
      "\n",
      "# given a sentence (list of Word Objects), returns phonetlicious score\n",
      "def phonetilicious_score(loWO, flatten=False, normalize=False):\n",
      "    if not loWO:\n",
      "        return 0\n",
      "    \n",
      "    phonevectors = map(lambda WO: WO[\"phonevector\"], loWO)\n",
      "    \n",
      "    if flatten:\n",
      "        # set each word's phoneme count to 0 or 1.\n",
      "        phonevectors = map(lambda v: map(lambda p: 0 if p == 0 else 1, v),  phonevectors)\n",
      "    \n",
      "    # sum up phonevectors for each word\n",
      "    vector_sum = reduce(lambda v,w: v + w, phonevectors)\n",
      "    \n",
      "    score = 0.0\n",
      "\n",
      "    phone_count = 0\n",
      "    for n in vector_sum:\n",
      "        if n > 0:\n",
      "            score += n * n * n  # recurring phones get cubed points\n",
      "            phone_count += n\n",
      "    \n",
      "    if normalize:\n",
      "        score /= phone_count\n",
      "        \n",
      "    # count repeated words\n",
      "    # no, this isn't the place to do this, because by now we've already maximized one example for each phoneme\n",
      "    # do this earlier\n",
      "    \n",
      "    words = map(lambda WO: WO[\"word_lower\"], loWO)\n",
      "    word_doubles = 0\n",
      "    for word in words:\n",
      "        extras = words.count(word) - 1\n",
      "        if extras > 0:\n",
      "            word_doubles += extras\n",
      "    score /= (0.2 * (word_doubles+1)) #severely penalized for extra words\n",
      "    \n",
      "    return score \n",
      "    \n",
      "\"\"\"\n",
      "Word Object looks like:\n",
      ",\n",
      "WO = {    'word_original': 'Truly',\n",
      "          'word_lower': 'truly'\n",
      "        'pos': 'RB',\n",
      "        'ignore': true,\n",
      "        'phonevector': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0])}\n",
      "\"\"\"\n",
      "# given a phone ph, and list of Word Objects, returns the Word Object with max occurance of ph\n",
      "# try to avoid words on blacklist (word_lower)\n",
      "def maxphone(ph, loWO, blacklist=[]):\n",
      "    if blacklist: \n",
      "        new_loWO = [WO for WO in loWO if WO[\"word_lower\"] not in blacklist]\n",
      "        if len(new_loWO) == 0:\n",
      "            new_loWO = loWO\n",
      "    else:\n",
      "        new_loWO = loWO\n",
      "    \n",
      "    result = max(new_loWO, key = lambda x: x['phonevector'][Phonemes.index(ph)])\n",
      "    \n",
      "    return result\n",
      "\n",
      "        \n",
      "\n",
      "# format new word with the capitalization of the old word\n",
      "def format_capitalization(new_word, old_word):\n",
      "    if old_word.capitalize() == old_word:\n",
      "        return new_word.capitalize()\n",
      "    elif old_word.upper() == old_word:\n",
      "        return new_word.upper()\n",
      "    elif old_word.lower() == old_word:\n",
      "        return new_word.lower()\n",
      "    else:\n",
      "        # if it's none of these, then it's something weirder\n",
      "        return new_word\n",
      "    \n",
      "    \n",
      "def pos_topia2wordnet(pos):\n",
      "    x = pos[:1]\n",
      "    if x == \"N\":\n",
      "        return \"n\"\n",
      "    elif x == \"V\":\n",
      "        return \"v\"\n",
      "    elif x == \"J\":\n",
      "        return \"a\"\n",
      "    elif x == \"R\":\n",
      "        return \"r\"\n",
      "    else:\n",
      "        return x.lower()\n",
      "    \n",
      "import random\n",
      "\n",
      "def algo(text, \n",
      "         alliteration_only = False, \n",
      "         number_of_examples = 4, \n",
      "         force_phoneme = [], \n",
      "         flatten = False, \n",
      "         split_by = \"sentences\", # \"sentences\" or \"punctuation\"\n",
      "         normalize = False, \n",
      "         try_to_avoid_dups = False, \n",
      "         word_additions = [],\n",
      "         swap_probability = 1.0,\n",
      "         hyper_profanity = False,\n",
      "         ignore_numbers = True,\n",
      "         debug = False):\n",
      "    \n",
      "    DEBUG = debug\n",
      "    \n",
      "    out = []\n",
      "    \n",
      "    if split_by == \"punctuation\":\n",
      "        sentences = re.split(r'( *[\\.\\?\\;\\,\\:\\\u2014\\n!][\\'\"\\)\\]]* *)', text)\n",
      "    elif split_by == \"lines\":\n",
      "        sentences = re.split(r'(\\n)', text)\n",
      "    else:\n",
      "        sentences = sent_tokenize(text)\n",
      "    \n",
      "    if hyper_profanity:\n",
      "        word_additions += PROFANITY\n",
      "    \n",
      "    if DEBUG:\n",
      "        print sentences\n",
      "        \n",
      "    for sentence in sentences: \n",
      "        \n",
      "        # should be tokenized with part of speech, and commonness\n",
      "        words = extract_words(sentence)\n",
      "        \n",
      "        if not words:\n",
      "            continue #empty sentence\n",
      "        \n",
      "        if DEBUG:\n",
      "            print words\n",
      "        # which words are we keeping?\n",
      "        \n",
      "        # just nouns, verbs, adjectives, adverbs\n",
      "        pos = [\"N\",\"V\",\"J\",\"R\"]\n",
      "        \n",
      "        for w in words:\n",
      "            w[\"ignore\"] = (w['word_lower'] in stop) or not (w['pos'][:1] in pos)\n",
      "            w[\"phonevector\"] = phonevector(phones(w[\"word_lower\"]), alliteration_only)\n",
      "        \n",
      "        #if DEBUG: \n",
      "            #print words\n",
      "        \n",
      "        \n",
      "        # for each word, find a list of related Word Objects\n",
      "        lolorWO = map(lambda w: \n",
      "                      [w] if w[\"ignore\"] else\n",
      "                      map(lambda rw: {\n",
      "                            \"word_lower\": rw, \n",
      "                            \"word_original\": w['word_original'],\n",
      "                            \"word_new\": format_capitalization(rw, w['word_original']),\n",
      "                            \"pos\": w['pos'],\n",
      "                            \"phonevector\": phonevector(phones(rw), alliteration_only)\n",
      "                        }, possible_words(w[\"word_lower\"],w['pos'], word_additions, swap_probability, ignore_numbers)),\n",
      "                    words) \n",
      "        \n",
      "        #if DEBUG: \n",
      "            #print lolorWO\n",
      "            \n",
      "        def generate_sentence_with_maxphone(p,lolorWO):\n",
      "            sentence = []\n",
      "            blacklist = []\n",
      "            for loWO in lolorWO:\n",
      "                mp = maxphone(p, loWO, blacklist)\n",
      "                if try_to_avoid_dups:\n",
      "                    blacklist.append(mp[\"word_lower\"])\n",
      "                sentence.append(mp)\n",
      "            return sentence\n",
      "            #sentence = map(lambda loWO: maxphone(p, loWO), lolorWO)\n",
      "            \n",
      "        \n",
      "        if len(force_phoneme) == 0:\n",
      "            phonemes_to_maximize = Phonemes\n",
      "        else:\n",
      "            phonemes_to_maximize = force_phoneme\n",
      "        loS = map(lambda p: generate_sentence_with_maxphone(p, lolorWO), phonemes_to_maximize)\n",
      "        \n",
      "        #if DEBUG:\n",
      "            #print loS\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        sentences_with_score = map(lambda s: {'sentence': sentencify(s, sentence), 'score': phonetilicious_score(s, flatten, normalize)}, loS)\n",
      "        \n",
      "        indexes = unique(sentences_with_score, return_index=True)[1]\n",
      "        sentences_unique = [sentences_with_score[index] for index in sorted(indexes)]\n",
      "        # unique array, still sorted by score. \n",
      "        # http://stackoverflow.com/questions/12926898/numpy-unique-without-sort\n",
      "        \n",
      "        best_sentences = sorted(sentences_unique, key=lambda s: s['score'], reverse=True)\n",
      "        \n",
      "        if DEBUG:\n",
      "            for s in best_sentences:\n",
      "                print s\n",
      "                \n",
      "        # trim down number of examples\n",
      "        best_sentences = best_sentences[:number_of_examples]\n",
      "                \n",
      "        if DEBUG:\n",
      "            print \"\\nBEST SENTENCE:\"\n",
      "            print best_sentences[0]\n",
      "            print \"\\n\"\n",
      "        \n",
      "        for s in best_sentences:\n",
      "            out.append(s['sentence'])\n",
      "        \n",
      "    return out\n",
      "\n",
      "# punctuation = (';', ':', ',', '.', '!', '?','...','\"',\"'\") \n",
      "\n",
      "# given a list of Word Objects, returns sentence\n",
      "def sentencify(loWO, original_sentence):\n",
      "    \n",
      "    \"\"\"\n",
      "    # remove unnecessary space around punctuation\n",
      "    i = 0\n",
      "    while i<len(words):\n",
      "        if i>0 and (words[i] in PUNCTUATION):\n",
      "            words[i-1] = words[i-1] + words[i]\n",
      "            words.pop(i)\n",
      "        else:\n",
      "            i+=1\n",
      "    \"\"\"\n",
      "\n",
      "    \n",
      "    # this is wrong, \n",
      "    # because if our replacements are [B->C, C->D] \n",
      "    # this will incorrectly go  A B C => A C C => A D C  \n",
      "    # instead of                A B C => A C D\n",
      "    x = 0\n",
      "    for WO in loWO:\n",
      "        original_sentence = original_sentence[:x] + original_sentence[x:].replace(WO['word_original'], WO['word_new'], 1)\n",
      "        x += len(WO['word_new'])\n",
      "    \n",
      "    words = original_sentence.split(\" \")\n",
      "        \n",
      "    # fix an; \"a egg\"  -> \"an egg\"\n",
      "    i = 0\n",
      "    for (i,word) in enumerate(words):\n",
      "        if (i+1) < len(words):\n",
      "            vowel_word = words[i+1][:1].lower() in [\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
      "            if word.lower() == \"a\" and vowel_word:\n",
      "                words[i] = format_capitalization(\"an\",word)\n",
      "            elif word.lower() == \"an\" and not vowel_word:\n",
      "                words[i] = format_capitalization(\"a\",word)\n",
      "    \n",
      "    return \" \".join(words)\n",
      "\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "lemmatizer = WordNetLemmatizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from pattern.en.wordlist import PROFANITY\n",
      "PROFANITY = []\n",
      "with open(\"profanity.txt\") as f:\n",
      "    for line in f:\n",
      "        PROFANITY.append(line.strip())\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# given a word, return list of related words\n",
      "# pos = part of speech\n",
      "# word_additions = extra words to choose from (ex: profanity)\n",
      "# addition_probably = probably that we add the word_additions to the new word list (0 to 1)\n",
      "def possible_words(word, pos, word_additions, swap_probability=1.0, ignore_numbers = True):\n",
      "    \n",
      "    if(swap_probability <= random.random()):\n",
      "        return [word]\n",
      "    \n",
      "    if ignore_numbers and (word.strip().isdigit()):\n",
      "        return [word]\n",
      "    \n",
      "    pos_wordnet = pos_topia2wordnet(pos)\n",
      "   \n",
      "    # synonyms\n",
      "    #lemma = lemmatizer.lemmatize(word, pos) \n",
      "    syn_sets = wn.synsets(word,pos_wordnet)\n",
      "\n",
      "    all_syn_sets = syn_sets\n",
      "    \n",
      "    #add hypernyms\n",
      "    for syn_set in syn_sets:\n",
      "        all_syn_sets += syn_set.hypernyms()\n",
      "        #all_syn_sets += syn_set.hyponyms()\n",
      "        \n",
      "    words = []\n",
      "\n",
      "    # TODO: pick synsets based on part of speech\n",
      "    for syn_set in all_syn_sets:\n",
      "        for w in syn_set.lemma_names():\n",
      "            if w not in words:\n",
      "                if w and (\"_\" not in w) and (w is not word):  # for now, ignore multi-word synonyms\n",
      "                    words.append(w)\n",
      "            elif len(words) >= max_synonyms:    \n",
      "                break\n",
      "        if len(words) >= max_synonyms:    \n",
      "            break\n",
      "    if not words:\n",
      "        words = [word]\n",
      "        \n",
      "    def format_word(w):\n",
      "        w = justlowerletters(w)\n",
      "        if pos[:1]==\"V\":\n",
      "            w = conjugate_verb(w, pos)\n",
      "        if pos[:1]==\"N\":\n",
      "            w = conjugate_noun(w, pos)\n",
      "        return w\n",
      "\n",
      "    \n",
      "    if word_additions:\n",
      "        # add extra words. (ex. profanity)\n",
      "        words = word_additions[:]\n",
      "    \n",
      "    # mix it up\n",
      "    shuffle(words)\n",
      "    \n",
      "    # conjugate, lowercase, etc\n",
      "    words = map(format_word, words)\n",
      "    return words\n",
      "    \n",
      "# given a word, return list of phones\n",
      "def phones(word):\n",
      "    if word in prondict:\n",
      "        return prondict[word][0]\n",
      "    else: # word is not in cmudict, return empty list\n",
      "        return []\n",
      "    \n",
      "# given a list of phones, return phone vector.\n",
      "def phonevector(lop, alliteration_only):\n",
      "    vector = [0]*num_phonemes\n",
      "\n",
      "    for p in lop:\n",
      "        p = p[:2] # ignore stress markers\n",
      "        vector[Phonemes.index(p)] += 1 #= 1 to flatten phonemes\n",
      "        if alliteration_only: \n",
      "            # only look at the first letter\n",
      "            break\n",
      "        \n",
      "    return array(vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pattern.en import conjugate, pluralize, singularize\n",
      "\n",
      "def conjugate_noun(noun, pos):\n",
      "    if pos==\"NNS\" or pos ==\"NNPS\":\n",
      "        return str(pluralize(noun))\n",
      "    elif pos==\"NN\" or pos ==\"NNP\":\n",
      "        return str(singularize(noun))\n",
      "    else:\n",
      "        return noun\n",
      "    \n",
      "# conjugate verb into new pos\n",
      "def conjugate_verb(verb, pos):\n",
      "    if pos[:1]==\"V\":\n",
      "        # verb which isn't be, do, have, or will/would/can/could\n",
      "        # we need to conjugate\n",
      "        if pos[2:3]==\"B\":\n",
      "            conj = conjugate(verb, tense = \"infinitive\")\n",
      "        elif pos[2:3]==\"D\":\n",
      "            conj = conjugate(verb, tense = \"past\")\n",
      "        elif pos[2:3]==\"G\":\n",
      "            conj = conjugate(verb, aspect = \"progressive\")\n",
      "        elif pos[2:3]==\"I\":\n",
      "            conj = conjugate(verb, tense = \"infinitive\")\n",
      "        elif pos[2:3]==\"N\":\n",
      "            conj = conjugate(verb, tense = \"past\", aspect=\"progressive\")\n",
      "        elif pos[2:3]==\"Z\":\n",
      "            conj = conjugate(verb, tense = \"present\", person = 3, number = \"singular\")\n",
      "        else:\n",
      "            conj = verb\n",
      "    return str(conj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"taoteching.txt\",\"r\")\n",
      "tao = f.read()\n",
      "f.close()\n",
      "\n",
      "def tao_rmx(text):\n",
      "    rmx = algo(text,\n",
      "              alliteration_only = True, \n",
      "              number_of_examples = 1, \n",
      "              split_by = \"lines\",\n",
      "              force_phoneme = [], # Consonants, Vowels, [], [\"K\"]\n",
      "              word_additions = [],\n",
      "              swap_probability = 0.7,\n",
      "              hyper_profanity = True,\n",
      "              ignore_numbers = True,\n",
      "              flatten = False, \n",
      "              normalize = False, \n",
      "              try_to_avoid_dups = True,\n",
      "              debug = True)\n",
      "    linguistic_nectar = \"\\n\".join(rmx)\n",
      "    return linguistic_nectar\n",
      "\n",
      "\n",
      "body = tao_rmx(tao)\n",
      "\n",
      "f = open(\"taoteching-profanity-remixK.txt\",\"w+\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f.write(body)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pattern.en import conjugate, pluralize, singularize\n",
      "\n",
      "text = \"\"\"\n",
      "National Novel Generation Month - based on an idea I tweeted on a whim. This is the 2014 edition, see here for 2013.\n",
      "The Goal\n",
      "Spend the month of November writing code that generates a novel of 50k+ words \n",
      "\n",
      "The Rules\n",
      "\n",
      "The only rule is that you share at least one novel and also your source code at the end.\n",
      "\n",
      "The source code does not have to be licensed in a particular way, so long as you share it. The code itself does not need to be on GitHub, either. I'm just using this repo as a place to organize the community.\n",
      "\n",
      "The \"novel\" is defined however you want. It could be 50,000 repetitions of the word \"meow\". It could literally grab a random novel from Project Gutenberg. It doesn't matter, as long as it's 50k+ words.\n",
      "\n",
      "Please try to respect copyright. I'm not going to police it, as ultimately it's on your head if you want to just copy/paste a Stephen King novel or whatever, but the most useful/interesting implementations are going to be ones that don't engender lawsuits.\n",
      "\n",
      "This activity starts at 12:01am GMT on Nov 1st and ends at 12:01am GMT Dec 1st.\n",
      "\n",
      "How to Participate\n",
      "\n",
      "Open an issue on this repo and declare your intent to participate. You may continually update the issue as you work over the course of the month. Feel free to post dev diaries, sample output, etc.\n",
      "\n",
      "Also feel free to comment on other participants' issues.\n",
      "\n",
      "Resources\n",
      "\n",
      "There's an open issue where you can add resources (libraries, corpuses, APIs, techniques, etc).\n",
      "\n",
      "There are already a ton of resources on the old resources thread for the 2013 edition.\n",
      "\n",
      "You might want to check out corpora, a repository of public domain lists of things: animals, foods, names, occupations, countries, etc.\n",
      "\n",
      "That's It\n",
      "\n",
      "So yeah. Have fun with this!\n",
      "\n",
      "\"\"\"\n",
      "rmx = algo(text,\n",
      "      alliteration_only = True, \n",
      "      number_of_examples = 1, \n",
      "      split_by = \"sentence\",\n",
      "      force_phoneme = Consonants, #Consonants,#, Vowels, [], [\"K\"]\n",
      "      word_additions = [],\n",
      "      hyper_profanity = False,\n",
      "      flatten = False, \n",
      "      normalize = False, \n",
      "      try_to_avoid_dups = True,\n",
      "      debug = True)\n",
      "output = \" \".join(rmx)\n",
      "print output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\\nNational Novel Generation Month - based on an idea I tweeted on a whim.', 'This is the 2014 edition, see here for 2013.', 'The Goal\\nSpend the month of November writing code that generates a novel of 50k+ words \\n\\nThe Rules\\n\\nThe only rule is that you share at least one novel and also your source code at the end.', 'The source code does not have to be licensed in a particular way, so long as you share it.', 'The code itself does not need to be on GitHub, either.', \"I'm just using this repo as a place to organize the community.\", 'The \"novel\" is defined however you want.', 'It could be 50,000 repetitions of the word \"meow\".', 'It could literally grab a random novel from Project Gutenberg.', \"It doesn't matter, as long as it's 50k+ words.\", 'Please try to respect copyright.', \"I'm not going to police it, as ultimately it's on your head if you want to just copy/paste a Stephen King novel or whatever, but the most useful/interesting implementations are going to be ones that don't engender lawsuits.\", 'This activity starts at 12:01am GMT on Nov 1st and ends at 12:01am GMT Dec 1st.', 'How to Participate\\n\\nOpen an issue on this repo and declare your intent to participate.', 'You may continually update the issue as you work over the course of the month.', 'Feel free to post dev diaries, sample output, etc.', \"Also feel free to comment on other participants' issues.\", \"Resources\\n\\nThere's an open issue where you can add resources (libraries, corpuses, APIs, techniques, etc).\", 'There are already a ton of resources on the old resources thread for the 2013 edition.', 'You might want to check out corpora, a repository of public domain lists of things: animals, foods, names, occupations, countries, etc.', \"That's It\\n\\nSo yeah.\", 'Have fun with this!\\n\\n']\n",
        "[{'ignore': False, 'word_original': 'National', 'word_lower': 'national', 'pos': 'NNP', 'word_new': 'National'}, {'ignore': False, 'word_original': 'Novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'Novel'}, {'ignore': False, 'word_original': 'Generation', 'word_lower': 'generation', 'pos': 'NNP', 'word_new': 'Generation'}, {'ignore': False, 'word_original': 'Month', 'word_lower': 'month', 'pos': 'NNP', 'word_new': 'Month'}, {'ignore': False, 'word_original': '-', 'word_lower': '-', 'pos': ':', 'word_new': '-'}, {'ignore': False, 'word_original': 'based', 'word_lower': 'based', 'pos': 'VBN', 'word_new': 'based'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'an', 'word_lower': 'an', 'pos': 'DT', 'word_new': 'an'}, {'ignore': False, 'word_original': 'idea', 'word_lower': 'idea', 'pos': 'NN', 'word_new': 'idea'}, {'ignore': False, 'word_original': 'I', 'word_lower': 'i', 'pos': 'PRP', 'word_new': 'I'}, {'ignore': False, 'word_original': 'tweeted', 'word_lower': 'tweeted', 'pos': 'NN', 'word_new': 'tweeted'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'whim', 'word_lower': 'whim', 'pos': 'NN', 'word_new': 'whim'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 379.99999999999994, 'sentence': '\\nCause Communication Quantity Entity - consumed on a content I tweeted on a caprice.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 259.99999999999994, 'sentence': '\\nSomeone Artifact Sex Entity - situated on a strain I tweeted on a state.'}\n",
        "{'score': 229.99999999999997, 'sentence': '\\nBeing Book Breeding Entity - based on a belief I tweeted on a whim.'}\n",
        "{'score': 229.99999999999997, 'sentence': '\\nSomeone Fiction Phase Entity - found on a figuring I tweeted on a feeling.'}\n",
        "{'score': 229.99999999999997, 'sentence': '\\nPerson Product Procreation Period - had on a persuasion I tweeted on a whim.'}\n",
        "{'score': 159.99999999999997, 'sentence': '\\nSomeone Artifact Dealing Entity - determined on a design I tweeted on a desire.'}\n",
        "{'score': 159.99999999999997, 'sentence': '\\nMortal Artifact Multiplication Measure - had on a mind I tweeted on a whim.'}\n",
        "{'score': 128.33333333333331, 'sentence': '\\nNational Novel Genesi Entity - had on a knowledge I tweeted on a notion.'}\n",
        "{'score': 99.99999999999999, 'sentence': '\\nWhole Artifact Happening Entity - had on a content I tweeted on a whim.'}\n",
        "{'score': 99.99999999999999, 'sentence': '\\nSomeone Artifact Transaction Entity - taken on a tune I tweeted on a whim.'}\n",
        "{'score': 69.99999999999999, 'sentence': '\\nSomeone Artifact Group Entity - had on a goal I tweeted on a whim.'}\n",
        "{'score': 69.99999999999999, 'sentence': '\\nSomeone Writing Reproduction Entity - had on a reckoning I tweeted on a whim.'}\n",
        "{'score': 68.33333333333333, 'sentence': '\\nSomeone Artifact Genesi Entity - located on a line I tweeted on a whim.'}\n",
        "{'score': 59.99999999999999, 'sentence': '\\nSomeone Artifact Generation Entity - had on a content I tweeted on a whim.'}\n",
        "{'score': 58.33333333333332, 'sentence': '\\nSomeone Artifact Genesi Entity - had on a content I tweeted on a whim.'}\n",
        "{'score': 58.33333333333332, 'sentence': '\\nSomeone Artifact Genesi Entity - had on a thought I tweeted on a whim.'}\n",
        "{'score': 58.33333333333332, 'sentence': '\\nUnit Artifact Genesi Entity - had on a content I tweeted on a whim.'}\n",
        "{'score': 38.33333333333333, 'sentence': '\\nSomeone Volume Genesi Entity - had on a view I tweeted on a whim.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 379.99999999999994, 'sentence': '\\nCause Communication Quantity Entity - consumed on a content I tweeted on a caprice.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'This', 'word_lower': 'this', 'pos': 'DT', 'word_new': 'This'}, {'ignore': False, 'word_original': 'is', 'word_lower': 'is', 'pos': 'VBZ', 'word_new': 'is'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': '2014', 'word_lower': '2014', 'pos': 'NN', 'word_new': '2014'}, {'ignore': False, 'word_original': 'edition', 'word_lower': 'edition', 'pos': 'NN', 'word_new': 'edition'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'see', 'word_lower': 'see', 'pos': 'VB', 'word_new': 'see'}, {'ignore': False, 'word_original': 'here', 'word_lower': 'here', 'pos': 'RB', 'word_new': 'here'}, {'ignore': False, 'word_original': 'for', 'word_lower': 'for', 'pos': 'IN', 'word_new': 'for'}, {'ignore': False, 'word_original': '2013.', 'word_lower': '2013.', 'pos': 'NN', 'word_new': '2013.'}]\n",
        "{'score': 185.0, 'sentence': 'This is the 2014 form, figure here for 2013.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 185.0, 'sentence': 'This is the 2014 whole, hear here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 grouping, gamble here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 content, call here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 number, notice here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 printing, play here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 sort, sight here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 type, trip here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 thought, think here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 version, visit here for 2013.'}\n",
        "{'score': 95.0, 'sentence': 'This is the 2014 work, wager here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, bet here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, change here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, date here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, call here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, journey here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, learn here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, move here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 version, reckon here for 2013.'}\n",
        "{'score': 65.0, 'sentence': 'This is the 2014 unit, call here for 2013.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 185.0, 'sentence': 'This is the 2014 form, figure here for 2013.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': 'Goal', 'word_lower': 'goal', 'pos': 'NN', 'word_new': 'Goal'}, {'ignore': False, 'word_original': 'Spend', 'word_lower': 'spend', 'pos': 'VB', 'word_new': 'Spend'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'month', 'word_lower': 'month', 'pos': 'NN', 'word_new': 'month'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'November', 'word_lower': 'november', 'pos': 'NNP', 'word_new': 'November'}, {'ignore': False, 'word_original': 'writing', 'word_lower': 'writing', 'pos': 'VBG', 'word_new': 'writing'}, {'ignore': False, 'word_original': 'code', 'word_lower': 'code', 'pos': 'NN', 'word_new': 'code'}, {'ignore': False, 'word_original': 'that', 'word_lower': 'that', 'pos': 'IN', 'word_new': 'that'}, {'ignore': False, 'word_original': 'generates', 'word_lower': 'generates', 'pos': 'VBZ', 'word_new': 'generates'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'novel'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': '50', 'word_lower': '50', 'pos': 'NN', 'word_new': '50'}, {'ignore': False, 'word_original': 'k', 'word_lower': 'k', 'pos': 'NN', 'word_new': 'k'}, {'ignore': False, 'word_original': '+', 'word_lower': '+', 'pos': 'SYM', 'word_new': '+'}, {'ignore': False, 'word_original': 'words', 'word_lower': 'words', 'pos': 'NNS', 'word_new': 'words'}, {'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': 'Rules', 'word_lower': 'rules', 'pos': 'NNP', 'word_new': 'Rules'}, {'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': 'only', 'word_lower': 'only', 'pos': 'RB', 'word_new': 'only'}, {'ignore': False, 'word_original': 'rule', 'word_lower': 'rule', 'pos': 'NN', 'word_new': 'rule'}, {'ignore': False, 'word_original': 'is', 'word_lower': 'is', 'pos': 'VBZ', 'word_new': 'is'}, {'ignore': False, 'word_original': 'that', 'word_lower': 'that', 'pos': 'IN', 'word_new': 'that'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'share', 'word_lower': 'share', 'pos': 'NN', 'word_new': 'share'}, {'ignore': False, 'word_original': 'at', 'word_lower': 'at', 'pos': 'IN', 'word_new': 'at'}, {'ignore': False, 'word_original': 'least', 'word_lower': 'least', 'pos': 'JJS', 'word_new': 'least'}, {'ignore': False, 'word_original': 'one', 'word_lower': 'one', 'pos': 'CD', 'word_new': 'one'}, {'ignore': False, 'word_original': 'novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'novel'}, {'ignore': False, 'word_original': 'and', 'word_lower': 'and', 'pos': 'CC', 'word_new': 'and'}, {'ignore': False, 'word_original': 'also', 'word_lower': 'also', 'pos': 'RB', 'word_new': 'also'}, {'ignore': False, 'word_original': 'your', 'word_lower': 'your', 'pos': 'PRP$', 'word_new': 'your'}, {'ignore': False, 'word_original': 'source', 'word_lower': 'source', 'pos': 'NN', 'word_new': 'source'}, {'ignore': False, 'word_original': 'code', 'word_lower': 'code', 'pos': 'NN', 'word_new': 'code'}, {'ignore': False, 'word_original': 'at', 'word_lower': 'at', 'pos': 'IN', 'word_new': 'at'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'end', 'word_lower': 'end', 'pos': 'NN', 'word_new': 'end'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 849.074074074074, 'sentence': 'The Cognition\\nConsume the quantity of Amount creating code that creates a communication of 50kilobyte+ components \\n\\nThe Concept\\n\\nThe only construct is that you contribution at least one creation and besides your cause codification at the conclusion.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 602.4074074074074, 'sentence': 'The Part\\nPass the period of Amount publishing abstraction that produces a product of 50potassium+ paroles \\n\\nThe Pattern\\n\\nThe only principle is that you possession at least one production and besides your person writing at the place.'}\n",
        "{'score': 345.7407407407407, 'sentence': 'The Score\\nSpend the period of Amount saving abstraction that sires an unit of 50signaling+ scriptures \\n\\nThe Substance\\n\\nThe only state is that you stock at least one production and besides your seed writing at the someone.'}\n",
        "{'score': 291.2962962962963, 'sentence': 'The Destination\\nDeplete the period of Amount drawing abstraction that yields an unit of 50drug+ disputes \\n\\nThe Dominion\\n\\nThe only device is that you distribution at least one production and besides your document writing at the destruction.'}\n",
        "{'score': 279.0740740740741, 'sentence': 'The Act\\nDeplete the measure of Month marking abstraction that mothers an unit of 50matter+ messages \\n\\nThe Message\\n\\nThe only state is that you machine at least one production and besides your maker writing at the material.'}\n",
        "{'score': 235.74074074074073, 'sentence': 'The Region\\nDeplete the period of Amount writing abstraction that returns an unit of 50relation+ relations \\n\\nThe Rule\\n\\nThe only regulation is that you event at least one production and besides your root communication at the role.'}\n",
        "{'score': 202.4074074074074, 'sentence': 'The Location\\nDeplete the period of Amount lining abstraction that yields an unit of 50letter+ lyrics \\n\\nThe Law\\n\\nThe only state is that you event at least one production and likewise your germ writing at the last.'}\n",
        "{'score': 170.18518518518516, 'sentence': 'The Terminal\\nTransfer the period of Amount taping abstraction that yields an unit of 50green+ texts \\n\\nThe Concept\\n\\nThe only state is that you try at least one production and too your germ writing at the textile.'}\n",
        "{'score': 147.96296296296296, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields an unit of 50thou+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 147.96296296296296, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that generates an unit of 50g+ evidences \\n\\nThe Generalization\\n\\nThe only generality is that you event at least one production and besides your germ writing at the jock.'}\n",
        "{'score': 144.62962962962962, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that begets a boogreen of 50k+ bibles \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your beginning writing at the being.'}\n",
        "{'score': 137.96296296296296, 'sentence': 'The Finish\\nDeplete the period of Amount taping abstraction that fathers a fiction of 50green+ evidences \\n\\nThe Formula\\n\\nThe only state is that you event at least one production and besides your facility writing at the fabric.'}\n",
        "{'score': 137.96296296296296, 'sentence': 'The Unit\\nDeplete the period of Amount taping abstraction that yields an entity of 50yard+ units \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 130.1851851851852, 'sentence': 'The Knowledge\\nDeplete the period of Nov taping abstraction that yields a novel of 50number+ news \\n\\nThe Normal\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 125.74074074074073, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields an unit of 50green+ words \\n\\nThe Concept\\n\\nThe only state is that you wedge at least one production and besides your work writing at the ending.'}\n",
        "{'score': 121.29629629629629, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields an unit of 50thousand+ evidences \\n\\nThe Thought\\n\\nThe only state is that you event at least one production and besides your thing writing at the ending.'}\n",
        "{'score': 116.85185185185185, 'sentence': 'The Act\\nDeplete the period of Amount changing abstraction that yields an unit of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 116.85185185185185, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields an unit of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 115.74074074074073, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields an unit of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you share at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 114.62962962962962, 'sentence': 'The Goal\\nGive the period of Amount taping abstraction that gets an unit of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "{'score': 113.5185185185185, 'sentence': 'The Whole\\nDeplete the period of Amount holding abstraction that yields an unit of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the happening.'}\n",
        "{'score': 110.18518518518518, 'sentence': 'The Act\\nDeplete the period of Amount taping abstraction that yields a volume of 50green+ evidences \\n\\nThe Concept\\n\\nThe only state is that you event at least one production and besides your germ writing at the ending.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 849.074074074074, 'sentence': 'The Cognition\\nConsume the quantity of Amount creating code that creates a communication of 50kilobyte+ components \\n\\nThe Concept\\n\\nThe only construct is that you contribution at least one creation and besides your cause codification at the conclusion.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': 'source', 'word_lower': 'source', 'pos': 'NN', 'word_new': 'source'}, {'ignore': False, 'word_original': 'code', 'word_lower': 'code', 'pos': 'NN', 'word_new': 'code'}, {'ignore': False, 'word_original': 'does', 'word_lower': 'does', 'pos': 'VBZ', 'word_new': 'does'}, {'ignore': False, 'word_original': 'not', 'word_lower': 'not', 'pos': 'RB', 'word_new': 'not'}, {'ignore': False, 'word_original': 'have', 'word_lower': 'have', 'pos': 'VBP', 'word_new': 'have'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'be', 'word_lower': 'be', 'pos': 'VB', 'word_new': 'be'}, {'ignore': False, 'word_original': 'licensed', 'word_lower': 'licensed', 'pos': 'VBN', 'word_new': 'licensed'}, {'ignore': False, 'word_original': 'in', 'word_lower': 'in', 'pos': 'IN', 'word_new': 'in'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'particular', 'word_lower': 'particular', 'pos': 'JJ', 'word_new': 'particular'}, {'ignore': False, 'word_original': 'way', 'word_lower': 'way', 'pos': 'NN', 'word_new': 'way'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'so', 'word_lower': 'so', 'pos': 'RB', 'word_new': 'so'}, {'ignore': False, 'word_original': 'long', 'word_lower': 'long', 'pos': 'JJ', 'word_new': 'long'}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'share', 'word_lower': 'share', 'pos': 'NN', 'word_new': 'share'}, {'ignore': False, 'word_original': 'it', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'it'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 1170.0, 'sentence': 'The somebody communication does not have to be certified in a special selection, so recollective as you certificate it.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 720.0, 'sentence': 'The communicator communication does not have to be cleared in a finicky cognition, so recollective as you contribution it.'}\n",
        "{'score': 720.0, 'sentence': 'The point communication does not have to be passed in a peculiar path, so recollective as you portion it.'}\n",
        "{'score': 425.0, 'sentence': 'The writing communication does not have to be let in a finicky travel, so tenacious as you try it.'}\n",
        "{'score': 420.0, 'sentence': 'The document communication does not have to be let in a finicky deed, so recollective as you distribution it.'}\n",
        "{'score': 420.0, 'sentence': 'The facility communication does not have to be let in a finicky fashion, so farsighted as you endeavmy it.'}\n",
        "{'score': 420.0, 'sentence': 'The location communication does not have to be let in a finicky line, so long as you endeavmy it.'}\n",
        "{'score': 420.0, 'sentence': 'The mortal communication does not have to be moved in a finicky motion, so recollective as you machine it.'}\n",
        "{'score': 420.0, 'sentence': 'The writing communication does not have to be responded in a finicky room, so recollective as you relation it.'}\n",
        "{'score': 240.0, 'sentence': 'The work communication does not have to be let in a finicky way, so recollective as you wedge it.'}\n",
        "{'score': 175.0, 'sentence': 'The beginning communication does not have to be let in a finicky action, so recollective as you endeavmy it.'}\n",
        "{'score': 175.0, 'sentence': 'The whole communication does not have to be let in a finicky action, so recollective as you endeavmy it.'}\n",
        "{'score': 175.0, 'sentence': 'The knowledge communication does not have to be let in a finicky action, so recollective as you endeavmy it.'}\n",
        "{'score': 175.0, 'sentence': 'The unit communication does not have to be let in a finicky action, so recollective as you endeavmy it.'}\n",
        "{'score': 145.0, 'sentence': 'The writing communication does not have to be let in a finicky action, so recollective as you endeavmy it.'}\n",
        "{'score': 145.0, 'sentence': 'The generator communication does not have to be let in a finicky journey, so recollective as you endeavmy it.'}\n",
        "{'score': 145.0, 'sentence': 'The thing communication does not have to be let in a finicky thought, so recollective as you endeavmy it.'}\n",
        "{'score': 115.0, 'sentence': 'The writing communication does not have to be let in a finicky change, so recollective as you endeavmy it.'}\n",
        "{'score': 115.0, 'sentence': 'The writing communication does not have to be let in a finicky share, so recollective as you endeavmy it.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 1170.0, 'sentence': 'The somebody communication does not have to be certified in a special selection, so recollective as you certificate it.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': 'code', 'word_lower': 'code', 'pos': 'NN', 'word_new': 'code'}, {'ignore': False, 'word_original': 'itself', 'word_lower': 'itself', 'pos': 'PRP', 'word_new': 'itself'}, {'ignore': False, 'word_original': 'does', 'word_lower': 'does', 'pos': 'VBZ', 'word_new': 'does'}, {'ignore': False, 'word_original': 'not', 'word_lower': 'not', 'pos': 'RB', 'word_new': 'not'}, {'ignore': False, 'word_original': 'need', 'word_lower': 'need', 'pos': 'NN', 'word_new': 'need'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'be', 'word_lower': 'be', 'pos': 'VB', 'word_new': 'be'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'GitHub', 'word_lower': 'github', 'pos': 'NN', 'word_new': 'GitHub'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'either', 'word_lower': 'either', 'pos': 'DT', 'word_new': 'either'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 80.0, 'sentence': 'The communication itself does not demand to be on github, either.'}\n",
        "{'score': 80.0, 'sentence': 'The communication itself does not condition to be on github, either.'}\n",
        "{'score': 80.0, 'sentence': 'The communication itself does not necessary to be on github, either.'}\n",
        "{'score': 80.0, 'sentence': 'The writing itself does not requirement to be on github, either.'}\n",
        "{'score': 50.0, 'sentence': 'The communication itself does not motive to be on github, either.'}\n",
        "{'score': 50.0, 'sentence': 'The communication itself does not penury to be on github, either.'}\n",
        "{'score': 50.0, 'sentence': 'The communication itself does not state to be on github, either.'}\n",
        "{'score': 50.0, 'sentence': 'The communication itself does not thing to be on github, either.'}\n",
        "{'score': 50.0, 'sentence': 'The communication itself does not want to be on github, either.'}\n",
        "{'score': 45.0, 'sentence': 'The communication itself does not indigence to be on github, either.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 80.0, 'sentence': 'The communication itself does not demand to be on github, either.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'I', 'word_lower': 'i', 'pos': 'PRP', 'word_new': 'I'}, {'ignore': False, 'word_original': \"'m\", 'word_lower': \"'m\", 'pos': 'VBP', 'word_new': \"'m\"}, {'ignore': False, 'word_original': 'just', 'word_lower': 'just', 'pos': 'RB', 'word_new': 'just'}, {'ignore': False, 'word_original': 'using', 'word_lower': 'using', 'pos': 'VBG', 'word_new': 'using'}, {'ignore': False, 'word_original': 'this', 'word_lower': 'this', 'pos': 'DT', 'word_new': 'this'}, {'ignore': False, 'word_original': 'repo', 'word_lower': 'repo', 'pos': 'NN', 'word_new': 'repo'}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'place', 'word_lower': 'place', 'pos': 'NN', 'word_new': 'place'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'organize', 'word_lower': 'organize', 'pos': 'VB', 'word_new': 'organize'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'community', 'word_lower': 'community', 'pos': 'NN', 'word_new': 'community'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 425.0, 'sentence': \"I'm just having this repo as a home to handle the harmony.\"}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 425.0, 'sentence': \"I'm just consuming this repo as a country to command the coalition.\"}\n",
        "{'score': 425.0, 'sentence': \"I'm just taking this repo as a tract to mastermind the territory.\"}\n",
        "{'score': 245.0, 'sentence': \"I'm just moving this repo as a music to mastermind the harmony.\"}\n",
        "{'score': 245.0, 'sentence': \"I'm just practicing this repo as a place to pose the harmony.\"}\n",
        "{'score': 240.0, 'sentence': \"I'm just utilising this repo as a deed to deal the dominion.\"}\n",
        "{'score': 240.0, 'sentence': \"I'm just utilising this repo as a job to join the harmony.\"}\n",
        "{'score': 240.0, 'sentence': \"I'm just utilising this repo as a residence to mastermind the relation.\"}\n",
        "{'score': 240.0, 'sentence': \"I'm just utilising this repo as a station to set the state.\"}\n",
        "{'score': 155.0, 'sentence': \"I'm just working this repo as a work to mastermind the harmony.\"}\n",
        "{'score': 150.0, 'sentence': \"I'm just utilising this repo as a function to form the harmony.\"}\n",
        "{'score': 150.0, 'sentence': \"I'm just utilising this repo as a location to lay the harmony.\"}\n",
        "{'score': 150.0, 'sentence': \"I'm just using this repo as a noesi to unionize the harmony.\"}\n",
        "{'score': 120.0, 'sentence': \"I'm just utilising this repo as a berth to mastermind the harmony.\"}\n",
        "{'score': 120.0, 'sentence': \"I'm just utilising this repo as a knowledge to mastermind the harmony.\"}\n",
        "{'score': 120.0, 'sentence': \"I'm just utilising this repo as a shoe to mastermind the harmony.\"}\n",
        "{'score': 120.0, 'sentence': \"I'm just utilising this repo as a vicinity to mastermind the harmony.\"}\n",
        "{'score': 115.0, 'sentence': \"I'm just utilising this repo as a noesi to mastermind the harmony.\"}\n",
        "{'score': 115.0, 'sentence': \"I'm just utilising this repo as a noesi to mastermind the group.\"}\n",
        "{'score': 115.0, 'sentence': \"I'm just utilising this repo as a noesi to think the harmony.\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 425.0, 'sentence': \"I'm just having this repo as a home to handle the harmony.\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'The', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'The'}, {'ignore': False, 'word_original': '\"', 'word_lower': '\"', 'pos': '\"', 'word_new': '\"'}, {'ignore': False, 'word_original': 'novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'novel'}, {'ignore': False, 'word_original': '\"', 'word_lower': '\"', 'pos': '\"', 'word_new': '\"'}, {'ignore': False, 'word_original': 'is', 'word_lower': 'is', 'pos': 'VBZ', 'word_new': 'is'}, {'ignore': False, 'word_original': 'defined', 'word_lower': 'defined', 'pos': 'VBN', 'word_new': 'defined'}, {'ignore': False, 'word_original': 'however', 'word_lower': 'however', 'pos': 'RB', 'word_new': 'however'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'want', 'word_lower': 'want', 'pos': 'VBP', 'word_new': 'want'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"book\" is been however you be.'}\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"creation\" is qualified however you convey.'}\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"novel\" is decided nonetheless you need.'}\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"writing\" is remembered however you request.'}\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"abstraction\" is selected still you seek.'}\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"unit\" is decided yet you want.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"abstraction\" is decided however you desire.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"fiction\" is fixed however you want.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"whole\" is decided however you want.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"abstraction\" is limited however you lack.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"abstraction\" is marked however you miss.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"production\" is decided however you pass.'}\n",
        "{'score': 21.666666666666664, 'sentence': 'The \"abstraction\" is taken however you transfer.'}\n",
        "{'score': 11.666666666666664, 'sentence': 'The \"abstraction\" is chosen however you want.'}\n",
        "{'score': 11.666666666666664, 'sentence': 'The \"abstraction\" is decided however you want.'}\n",
        "{'score': 11.666666666666664, 'sentence': 'The \"abstraction\" is shown however you want.'}\n",
        "{'score': 11.666666666666664, 'sentence': 'The \"volume\" is decided however you want.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 51.66666666666666, 'sentence': 'The \"book\" is been however you be.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'It', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'It'}, {'ignore': False, 'word_original': 'could', 'word_lower': 'could', 'pos': 'MD', 'word_new': 'could'}, {'ignore': False, 'word_original': 'be', 'word_lower': 'be', 'pos': 'VB', 'word_new': 'be'}, {'ignore': False, 'word_original': '50,000', 'word_lower': '50,000', 'pos': 'NN', 'word_new': '50,000'}, {'ignore': False, 'word_original': 'repetitions', 'word_lower': 'repetitions', 'pos': 'NNS', 'word_new': 'repetitions'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'word', 'word_lower': 'word', 'pos': 'NN', 'word_new': 'word'}, {'ignore': False, 'word_original': '\"', 'word_lower': '\"', 'pos': '\"', 'word_new': '\"'}, {'ignore': False, 'word_original': 'meow', 'word_lower': 'meow', 'pos': 'NN', 'word_new': 'meow'}, {'ignore': False, 'word_original': '\".', 'word_lower': '\".', 'pos': '.', 'word_new': '\".'}]\n",
        "{'score': 340.0, 'sentence': 'It could be 50,000 continuances of the component \"cry\".'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 160.0, 'sentence': 'It could be 50,000 styles of the substance \"sound\".'}\n",
        "{'score': 70.0, 'sentence': 'It could be 50,000 deeds of the bid \"mew\".'}\n",
        "{'score': 70.0, 'sentence': 'It could be 50,000 deeds of the dedication \"mew\".'}\n",
        "{'score': 70.0, 'sentence': 'It could be 50,000 happenings of the word \"happening\".'}\n",
        "{'score': 70.0, 'sentence': 'It could be 50,000 deeds of the measure \"mew\".'}\n",
        "{'score': 70.0, 'sentence': 'It could be 50,000 repeats of the writing \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the word \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the logo \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the news \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the password \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the tiding \"mew\".'}\n",
        "{'score': 40.0, 'sentence': 'It could be 50,000 deeds of the unit \"mew\".'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 340.0, 'sentence': 'It could be 50,000 continuances of the component \"cry\".'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'It', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'It'}, {'ignore': False, 'word_original': 'could', 'word_lower': 'could', 'pos': 'MD', 'word_new': 'could'}, {'ignore': False, 'word_original': 'literally', 'word_lower': 'literally', 'pos': 'RB', 'word_new': 'literally'}, {'ignore': False, 'word_original': 'grab', 'word_lower': 'grab', 'pos': 'VB', 'word_new': 'grab'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'random', 'word_lower': 'random', 'pos': 'JJ', 'word_new': 'random'}, {'ignore': False, 'word_original': 'novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'novel'}, {'ignore': False, 'word_original': 'from', 'word_lower': 'from', 'pos': 'IN', 'word_new': 'from'}, {'ignore': False, 'word_original': 'Project', 'word_lower': 'project', 'pos': 'NNP', 'word_new': 'Project'}, {'ignore': False, 'word_original': 'Gutenberg', 'word_lower': 'gutenberg', 'pos': 'NN', 'word_new': 'Gutenberg'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 350.0, 'sentence': 'It could literally clutch a random communication from Cognition Gutenberg.'}\n",
        "{'score': 200.0, 'sentence': 'It could literally fascinate a random fiction from Labor Gutenberg.'}\n",
        "{'score': 200.0, 'sentence': 'It could literally relate a random writing from Labor Gutenberg.'}\n",
        "{'score': 170.0, 'sentence': 'It could literally pertain a random production from Projection Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random book from Labor Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random production from Labor Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally get a random production from Labor Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random whole from Labor Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random novel from Knowledge Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random volume from Labor Gutenberg.'}\n",
        "{'score': 110.0, 'sentence': 'It could literally clutch a random unit from Labor Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally clutch a random production from Deed Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally move a random production from Labor Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally seize a random production from Labor Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally touch a random production from Task Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally clutch a random production from Thought Gutenberg.'}\n",
        "{'score': 80.0, 'sentence': 'It could literally clutch a random production from Work Gutenberg.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 350.0, 'sentence': 'It could literally clutch a random communication from Cognition Gutenberg.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'It', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'It'}, {'ignore': False, 'word_original': 'doesn', 'word_lower': 'doesn', 'pos': 'NN', 'word_new': 'doesn'}, {'ignore': False, 'word_original': \"'t\", 'word_lower': \"'t\", 'pos': 'RB', 'word_new': \"'t\"}, {'ignore': False, 'word_original': 'matter', 'word_lower': 'matter', 'pos': 'NN', 'word_new': 'matter'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'long', 'word_lower': 'long', 'pos': 'JJ', 'word_new': 'long'}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'it', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'it'}, {'ignore': False, 'word_original': \"'s\", 'word_lower': \"'s\", 'pos': 'POS', 'word_new': \"'s\"}, {'ignore': False, 'word_original': '50', 'word_lower': '50', 'pos': 'NN', 'word_new': '50'}, {'ignore': False, 'word_original': 'k', 'word_lower': 'k', 'pos': 'NN', 'word_new': 'k'}, {'ignore': False, 'word_original': '+', 'word_lower': '+', 'pos': 'SYM', 'word_new': '+'}, {'ignore': False, 'word_original': 'words', 'word_lower': 'words', 'pos': 'NNS', 'word_new': 'words'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 45.0, 'sentence': \"It doesn't difficulty, as tenacious as it's 50drug+ disputes.\"}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 45.0, 'sentence': \"It doesn't condition, as tenacious as it's 50cause+ communications.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't value, as long as it's 50letter+ languages.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't moment, as tenacious as it's 50measure+ measures.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't knowledge, as tenacious as it's 50number+ news.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't problem, as tenacious as it's 50potassium+ paroles.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't writing, as tenacious as it's 50relation+ rows.\"}\n",
        "{'score': 45.0, 'sentence': \"It doesn't subject, as tenacious as it's 50signal+ statements.\"}\n",
        "{'score': 44.0, 'sentence': \"It doesn't topic, as tenacious as it's 50kb+ texts.\"}\n",
        "{'score': 27.0, 'sentence': \"It doesn't thing, as tenacious as it's 50thousand+ statements.\"}\n",
        "{'score': 27.0, 'sentence': \"It doesn't value, as tenacious as it's 50unit+ units.\"}\n",
        "{'score': 26.0, 'sentence': \"It doesn't wonder, as tenacious as it's 50kb+ watchwords.\"}\n",
        "{'score': 21.0, 'sentence': \"It doesn't value, as tenacious as it's 50thou+ statements.\"}\n",
        "{'score': 21.0, 'sentence': \"It doesn't value, as tenacious as it's 50grand+ statements.\"}\n",
        "{'score': 21.0, 'sentence': \"It doesn't value, as tenacious as it's 50g+ statements.\"}\n",
        "{'score': 20.0, 'sentence': \"It doesn't value, as tenacious as it's 50kb+ bibles.\"}\n",
        "{'score': 20.0, 'sentence': \"It doesn't value, as tenacious as it's 50kb+ statements.\"}\n",
        "{'score': 20.0, 'sentence': \"It doesn't value, as farsighted as it's 50kb+ statements.\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 45.0, 'sentence': \"It doesn't difficulty, as tenacious as it's 50drug+ disputes.\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'Please', 'word_lower': 'please', 'pos': 'VB', 'word_new': 'Please'}, {'ignore': False, 'word_original': 'try', 'word_lower': 'try', 'pos': 'VB', 'word_new': 'try'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'respect', 'word_lower': 'respect', 'pos': 'NN', 'word_new': 'respect'}, {'ignore': False, 'word_original': 'copyright', 'word_lower': 'copyright', 'pos': 'NN', 'word_new': 'copyright'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 325.0, 'sentence': 'Delight decide to deed document.'}\n",
        "{'score': 325.0, 'sentence': 'Care canvas to conduct construct.'}\n",
        "{'score': 325.0, 'sentence': 'Please provoke to position paper.'}\n",
        "{'score': 145.0, 'sentence': 'Like resolve to regard writing.'}\n",
        "{'score': 145.0, 'sentence': 'Satisfy strain to civility idea.'}\n",
        "{'score': 145.0, 'sentence': 'Like taste to trait idea.'}\n",
        "{'score': 55.0, 'sentence': 'Like fire to feeling idea.'}\n",
        "{'score': 55.0, 'sentence': 'Like have to heart idea.'}\n",
        "{'score': 55.0, 'sentence': 'Like upset to laurel idea.'}\n",
        "{'score': 55.0, 'sentence': 'Like melt to manner idea.'}\n",
        "{'score': 55.0, 'sentence': 'Like think to regard thought.'}\n",
        "{'score': 55.0, 'sentence': 'Want wear to regard idea.'}\n",
        "{'score': 25.0, 'sentence': 'Like upset to behavior idea.'}\n",
        "{'score': 25.0, 'sentence': 'Like change to regard idea.'}\n",
        "{'score': 25.0, 'sentence': 'Like upset to regard idea.'}\n",
        "{'score': 25.0, 'sentence': 'Gratify upset to regard idea.'}\n",
        "{'score': 25.0, 'sentence': 'Like judge to regard idea.'}\n",
        "{'score': 25.0, 'sentence': 'Like upset to knowledge idea.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 325.0, 'sentence': 'Delight decide to deed document.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'I', 'word_lower': 'i', 'pos': 'PRP', 'word_new': 'I'}, {'ignore': False, 'word_original': \"'m\", 'word_lower': \"'m\", 'pos': 'VBP', 'word_new': \"'m\"}, {'ignore': False, 'word_original': 'not', 'word_lower': 'not', 'pos': 'RB', 'word_new': 'not'}, {'ignore': False, 'word_original': 'going', 'word_lower': 'going', 'pos': 'VBG', 'word_new': 'going'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'police', 'word_lower': 'police', 'pos': 'NN', 'word_new': 'police'}, {'ignore': False, 'word_original': 'it', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'it'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'ultimately', 'word_lower': 'ultimately', 'pos': 'RB', 'word_new': 'ultimately'}, {'ignore': False, 'word_original': 'it', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'it'}, {'ignore': False, 'word_original': \"'s\", 'word_lower': \"'s\", 'pos': 'POS', 'word_new': \"'s\"}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'your', 'word_lower': 'your', 'pos': 'PRP$', 'word_new': 'your'}, {'ignore': False, 'word_original': 'head', 'word_lower': 'head', 'pos': 'NN', 'word_new': 'head'}, {'ignore': False, 'word_original': 'if', 'word_lower': 'if', 'pos': 'IN', 'word_new': 'if'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'want', 'word_lower': 'want', 'pos': 'VBP', 'word_new': 'want'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'just', 'word_lower': 'just', 'pos': 'RB', 'word_new': 'just'}, {'ignore': False, 'word_original': 'copy', 'word_lower': 'copy', 'pos': 'NN', 'word_new': 'copy'}, {'ignore': False, 'word_original': '/paste', 'word_lower': '/paste', 'pos': 'NN', 'word_new': '/paste'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'Stephen', 'word_lower': 'stephen', 'pos': 'NNP', 'word_new': 'Stephen'}, {'ignore': False, 'word_original': 'King', 'word_lower': 'king', 'pos': 'NNP', 'word_new': 'King'}, {'ignore': False, 'word_original': 'novel', 'word_lower': 'novel', 'pos': 'NN', 'word_new': 'novel'}, {'ignore': False, 'word_original': 'or', 'word_lower': 'or', 'pos': 'CC', 'word_new': 'or'}, {'ignore': False, 'word_original': 'whatever', 'word_lower': 'whatever', 'pos': 'WDT', 'word_new': 'whatever'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'but', 'word_lower': 'but', 'pos': 'CC', 'word_new': 'but'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'most', 'word_lower': 'most', 'pos': 'RBS', 'word_new': 'most'}, {'ignore': False, 'word_original': 'useful', 'word_lower': 'useful', 'pos': 'JJ', 'word_new': 'useful'}, {'ignore': False, 'word_original': '/interesting', 'word_lower': '/interesting', 'pos': 'NN', 'word_new': '/interesting'}, {'ignore': False, 'word_original': 'implementations', 'word_lower': 'implementations', 'pos': 'NNS', 'word_new': 'implementations'}, {'ignore': False, 'word_original': 'are', 'word_lower': 'are', 'pos': 'VBP', 'word_new': 'are'}, {'ignore': False, 'word_original': 'going', 'word_lower': 'going', 'pos': 'VBG', 'word_new': 'going'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'be', 'word_lower': 'be', 'pos': 'VB', 'word_new': 'be'}, {'ignore': False, 'word_original': 'ones', 'word_lower': 'ones', 'pos': 'NNS', 'word_new': 'ones'}, {'ignore': False, 'word_original': 'that', 'word_lower': 'that', 'pos': 'IN', 'word_new': 'that'}, {'ignore': False, 'word_original': 'don', 'word_lower': 'don', 'pos': 'VB', 'word_new': 'don'}, {'ignore': False, 'word_original': \"'t\", 'word_lower': \"'t\", 'pos': 'RB', 'word_new': \"'t\"}, {'ignore': False, 'word_original': 'engender', 'word_lower': 'engender', 'pos': 'VB', 'word_new': 'engender'}, {'ignore': False, 'word_original': 'lawsuits', 'word_lower': 'lawsuits', 'pos': 'NNS', 'word_new': 'lawsuits'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 664.5454545454545, 'sentence': \"I'm not comparing to constabulary it, as finally it's on your coil if you convey to just content/paste a Stephen Competition communication or whatever, but the most utilitarian/interesting enforcements are consorting to be conceptions that don't cause causes.\"}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 533.6363636363636, 'sentence': \"I'm not starting to office it, as finally it's on your somebody if you search to just substance/paste a Stephen Stuff unit or whatever, but the most utilitarian/interesting enforcements are selecting to be singles that don't spawn suits.\"}\n",
        "{'score': 435.38461538461536, 'sentence': \"I'm not belonging to bureau it, as finally it's on your brute if you be to just material/paste a Stephen Businessman book or whatever, but the most utilitarian/interesting enforcements are blending to be thoughts that don't breed causes.\"}\n",
        "{'score': 410.9090909090909, 'sentence': \"I'm not deciding to office it, as finally it's on your drumhead if you displace to just material/paste a Stephen Distinction unit or whatever, but the most utilitarian/interesting deeds are disappearing to be digits that don't do causes.\"}\n",
        "{'score': 410.9090909090909, 'sentence': \"I'm not matching to office it, as finally it's on your mind if you miss to just material/paste a Stephen Magnate unit or whatever, but the most utilitarian/interesting enforcements are moving to be measures that don't make causes.\"}\n",
        "{'score': 402.7272727272727, 'sentence': \"I'm not terminating to office it, as finally it's on your topic if you transmit to just transcript/paste a Stephen Treater unit or whatever, but the most utilitarian/interesting enforcements are turning to be thoughts that don't spawn causes.\"}\n",
        "{'score': 301.8181818181818, 'sentence': \"I'm not functioning to force it, as finally it's on your fauna if you displace to just material/paste a Stephen Competition fiction or whatever, but the most utilitarian/interesting enforcements are fitting to be figures that don't father causes.\"}\n",
        "{'score': 299.09090909090907, 'sentence': \"I'm not proceeding to personnel it, as finally it's on your privy if you pass to just material/paste a Stephen Portion production or whatever, but the most utilitarian/interesting enforcements are perishing to be thoughts that don't spawn proceedings.\"}\n",
        "{'score': 192.72727272727272, 'sentence': \"I'm not belonging to office it, as finally it's on your knowledge if you need to just material/paste a Stephen Negotiator novel or whatever, but the most utilitarian/interesting enforcements are blending to be numbers that don't spawn causes.\"}\n",
        "{'score': 187.27272727272725, 'sentence': \"I'm not belonging to unit it, as finally it's on your user if you displace to just material/paste a Stephen Competition abstraction or whatever, but the most utilitarian/interesting enforcements are blending to be units that don't spawn causes.\"}\n",
        "{'score': 181.8181818181818, 'sentence': \"I'm not living to law it, as finally it's on your leader if you lack to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are leaving to be thoughts that don't spawn lawsuits.\"}\n",
        "{'score': 181.8181818181818, 'sentence': \"I'm not rifling to office it, as finally it's on your root if you request to just writing/paste a Stephen Representative unit or whatever, but the most utilitarian/interesting enforcements are running to be thoughts that don't spawn causes.\"}\n",
        "{'score': 154.54545454545453, 'sentence': \"I'm not working to office it, as finally it's on your word if you want to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are blending to be ones that don't spawn causes.\"}\n",
        "{'score': 146.36363636363635, 'sentence': \"I'm not belonging to office it, as finally it's on your brute if you displace to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are blending to be thoughts that don't spawn causes.\"}\n",
        "{'score': 146.36363636363635, 'sentence': \"I'm not harmonizing to office it, as finally it's on your headway if you displace to just whole/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are happening to be wholes that don't spawn causes.\"}\n",
        "{'score': 127.27272727272727, 'sentence': \"I'm not belonging to office it, as finally it's on your juncture if you displace to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are blending to be thoughts that don't generate causes.\"}\n",
        "{'score': 124.54545454545453, 'sentence': \"I'm not going to group it, as finally it's on your brute if you displace to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are getting to be thoughts that don't get causes.\"}\n",
        "{'score': 121.81818181818181, 'sentence': \"I'm not belonging to office it, as finally it's on your theme if you displace to just material/paste a Stephen Competition unit or whatever, but the most utilitarian/interesting enforcements are blending to be thoughts that don't spawn causes.\"}\n",
        "{'score': 119.09090909090908, 'sentence': \"I'm not choosing to office it, as finally it's on your chief if you displace to just material/paste a Stephen Challenger unit or whatever, but the most utilitarian/interesting enforcements are changing to be thoughts that don't spawn causes.\"}\n",
        "{'score': 105.45454545454544, 'sentence': \"I'm not vanishing to office it, as finally it's on your brute if you displace to just material/paste a Stephen Competition volume or whatever, but the most utilitarian/interesting enforcements are blending to be thoughts that don't spawn causes.\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 664.5454545454545, 'sentence': \"I'm not comparing to constabulary it, as finally it's on your coil if you convey to just content/paste a Stephen Competition communication or whatever, but the most utilitarian/interesting enforcements are consorting to be conceptions that don't cause causes.\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'This', 'word_lower': 'this', 'pos': 'DT', 'word_new': 'This'}, {'ignore': False, 'word_original': 'activity', 'word_lower': 'activity', 'pos': 'NN', 'word_new': 'activity'}, {'ignore': False, 'word_original': 'starts', 'word_lower': 'starts', 'pos': 'VBZ', 'word_new': 'starts'}, {'ignore': False, 'word_original': 'at', 'word_lower': 'at', 'pos': 'IN', 'word_new': 'at'}, {'ignore': False, 'word_original': '12:01', 'word_lower': '12:01', 'pos': 'NN', 'word_new': '12:01'}, {'ignore': False, 'word_original': 'am', 'word_lower': 'am', 'pos': 'VBP', 'word_new': 'am'}, {'ignore': False, 'word_original': 'GMT', 'word_lower': 'gmt', 'pos': 'NN', 'word_new': 'GMT'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'Nov', 'word_lower': 'nov', 'pos': 'NNP', 'word_new': 'Nov'}, {'ignore': False, 'word_original': '1', 'word_lower': '1', 'pos': 'NN', 'word_new': '1'}, {'ignore': False, 'word_original': 'st', 'word_lower': 'st', 'pos': 'NN', 'word_new': 'st'}, {'ignore': False, 'word_original': 'and', 'word_lower': 'and', 'pos': 'CC', 'word_new': 'and'}, {'ignore': False, 'word_original': 'ends', 'word_lower': 'ends', 'pos': 'NNS', 'word_new': 'ends'}, {'ignore': False, 'word_original': 'at', 'word_lower': 'at', 'pos': 'IN', 'word_new': 'at'}, {'ignore': False, 'word_original': '12:01', 'word_lower': '12:01', 'pos': 'NN', 'word_new': '12:01'}, {'ignore': False, 'word_original': 'am', 'word_lower': 'am', 'pos': 'VBP', 'word_new': 'am'}, {'ignore': False, 'word_original': 'GMT', 'word_lower': 'gmt', 'pos': 'NN', 'word_new': 'GMT'}, {'ignore': False, 'word_original': 'Dec', 'word_lower': 'dec', 'pos': 'NNP', 'word_new': 'Dec'}, {'ignore': False, 'word_original': '1', 'word_lower': '1', 'pos': 'NN', 'word_new': '1'}, {'ignore': False, 'word_original': 'st', 'word_lower': 'st', 'pos': 'NN', 'word_new': 'st'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 131.8181818181818, 'sentence': 'This susceptibility startles at 12:01am GMT on Amount 1st and sections at 12:01am UT Space 1st.'}\n",
        "{'score': 66.36363636363636, 'sentence': 'This deed departs at 12:01am GMT on Amount 1st and divisions at 12:01am UT Dec 1st.'}\n",
        "{'score': 63.63636363636363, 'sentence': 'This entity moves at 12:01am GMT on Measure 1st and musics at 12:01am UT Month 1st.'}\n",
        "{'score': 63.18181818181818, 'sentence': 'This capability contends at 12:01am GMT on Quantity 1st and communications at 12:01am UT Declination 1st.'}\n",
        "{'score': 49.99999999999999, 'sentence': 'This entity leaves at 12:01am GMT on Amount 1st and lasts at 12:01am UT Location 1st.'}\n",
        "{'score': 49.99999999999999, 'sentence': 'This trait contends at 12:01am TIME on Amount 1st and textiles at 12:01am UT Declination 1st.'}\n",
        "{'score': 46.81818181818181, 'sentence': 'This entity plays at 12:01am GMT on Period 1st and places at 12:01am UT Declination 1st.'}\n",
        "{'score': 41.81818181818181, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and finishes at 12:01am UT Form 1st.'}\n",
        "{'score': 41.81818181818181, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and shares at 12:01am UT Shape 1st.'}\n",
        "{'score': 41.36363636363636, 'sentence': 'This entity begins at 12:01am GMT on Amount 1st and boundaries at 12:01am UT Declination 1st.'}\n",
        "{'score': 41.36363636363636, 'sentence': 'This entity gets at 12:01am GMT on Amount 1st and goals at 12:01am UT Declination 1st.'}\n",
        "{'score': 41.36363636363636, 'sentence': 'This entity jumps at 12:01am GMT on Amount 1st and jocks at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity changes at 12:01am GMT on Amount 1st and ideas at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and ideas at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and happenings at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on November 1st and knowledge at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and roles at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and thoughts at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity vies at 12:01am GMT on Amount 1st and ideas at 12:01am UT Declination 1st.'}\n",
        "{'score': 38.63636363636363, 'sentence': 'This entity contends at 12:01am GMT on Amount 1st and units at 12:01am UT Declination 1st.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 131.8181818181818, 'sentence': 'This susceptibility startles at 12:01am GMT on Amount 1st and sections at 12:01am UT Space 1st.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'How', 'word_lower': 'how', 'pos': 'WRB', 'word_new': 'How'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'Participate', 'word_lower': 'participate', 'pos': 'NN', 'word_new': 'Participate'}, {'ignore': False, 'word_original': 'Open', 'word_lower': 'open', 'pos': 'NNP', 'word_new': 'Open'}, {'ignore': False, 'word_original': 'an', 'word_lower': 'an', 'pos': 'DT', 'word_new': 'an'}, {'ignore': False, 'word_original': 'issue', 'word_lower': 'issue', 'pos': 'NN', 'word_new': 'issue'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'this', 'word_lower': 'this', 'pos': 'DT', 'word_new': 'this'}, {'ignore': False, 'word_original': 'repo', 'word_lower': 'repo', 'pos': 'NN', 'word_new': 'repo'}, {'ignore': False, 'word_original': 'and', 'word_lower': 'and', 'pos': 'CC', 'word_new': 'and'}, {'ignore': False, 'word_original': 'declare', 'word_lower': 'declare', 'pos': 'VB', 'word_new': 'declare'}, {'ignore': False, 'word_original': 'your', 'word_lower': 'your', 'pos': 'PRP$', 'word_new': 'your'}, {'ignore': False, 'word_original': 'intent', 'word_lower': 'intent', 'pos': 'NN', 'word_new': 'intent'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'participate', 'word_lower': 'participate', 'pos': 'VB', 'word_new': 'participate'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 244.99999999999997, 'sentence': 'How to Participate\\n\\nPart a person on this repo and proclaim your purport to act.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 244.99999999999997, 'sentence': 'How to Participate\\n\\nTournament a taking on this repo and tell your purport to act.'}\n",
        "{'score': 154.99999999999997, 'sentence': 'How to Participate\\n\\nRegion a relation on this repo and respond your purport to act.'}\n",
        "{'score': 144.99999999999997, 'sentence': 'How to Participate\\n\\nCountry a content on this repo and consent your communication to act.'}\n",
        "{'score': 144.99999999999997, 'sentence': 'How to Participate\\n\\nSurface a store on this repo and set your significance to act.'}\n",
        "{'score': 124.99999999999999, 'sentence': 'How to Participate\\n\\nPart a whole on this repo and hold your purport to act.'}\n",
        "{'score': 94.99999999999999, 'sentence': 'How to Participate\\n\\nPart a deed on this repo and declare your design to act.'}\n",
        "{'score': 94.99999999999999, 'sentence': 'How to Participate\\n\\nPart a mercantilism on this repo and move your message to act.'}\n",
        "{'score': 94.99999999999999, 'sentence': 'How to Participate\\n\\nPart a yield on this repo and declare your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a beginning on this repo and declare your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a content on this repo and declare your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a phenomenon on this repo and declare your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a content on this repo and judge your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a content on this repo and think your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a content on this repo and verbalize your purport to act.'}\n",
        "{'score': 84.99999999999999, 'sentence': 'How to Participate\\n\\nPart a work on this repo and declare your purport to act.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'How to Participate\\n\\nPart a gain on this repo and declare your goal to act.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'How to Participate\\n\\nLocation a content on this repo and lay your purport to act.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'How to Participate\\n\\nKnowledge a number on this repo and declare your purport to act.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 244.99999999999997, 'sentence': 'How to Participate\\n\\nPart a person on this repo and proclaim your purport to act.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'You', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'You'}, {'ignore': False, 'word_original': 'may', 'word_lower': 'may', 'pos': 'MD', 'word_new': 'may'}, {'ignore': False, 'word_original': 'continually', 'word_lower': 'continually', 'pos': 'RB', 'word_new': 'continually'}, {'ignore': False, 'word_original': 'update', 'word_lower': 'update', 'pos': 'VB', 'word_new': 'update'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'issue', 'word_lower': 'issue', 'pos': 'NN', 'word_new': 'issue'}, {'ignore': False, 'word_original': 'as', 'word_lower': 'as', 'pos': 'IN', 'word_new': 'as'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'work', 'word_lower': 'work', 'pos': 'NN', 'word_new': 'work'}, {'ignore': False, 'word_original': 'over', 'word_lower': 'over', 'pos': 'IN', 'word_new': 'over'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'course', 'word_lower': 'course', 'pos': 'NN', 'word_new': 'course'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'month', 'word_lower': 'month', 'pos': 'NN', 'word_new': 'month'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 141.66666666666666, 'sentence': 'You may continually communicate the commercialism as you cognition over the course of the quantity.'}\n",
        "{'score': 58.33333333333333, 'sentence': 'You may continually modify the matter as you line over the pedagogy of the month.'}\n",
        "{'score': 58.33333333333333, 'sentence': 'You may continually change the publication as you product over the pedagogy of the period.'}\n",
        "{'score': 41.666666666666664, 'sentence': 'You may continually change the dealing as you deed over the didactic of the quantity.'}\n",
        "{'score': 41.666666666666664, 'sentence': 'You may continually change the net as you knowledge over the nourishment of the quantity.'}\n",
        "{'score': 41.666666666666664, 'sentence': 'You may continually change the supplying as you study over the sustenance of the quantity.'}\n",
        "{'score': 41.666666666666664, 'sentence': 'You may continually change the work as you workplace over the way of the quantity.'}\n",
        "{'score': 38.333333333333336, 'sentence': 'You may continually change the unit as you line over the pedagogy of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the being as you line over the bed of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the publication as you line over the pedagogy of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the phenomenon as you line over the form of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the gain as you line over the grade of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the happening as you whole over the pedagogy of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the publication as you job over the pedagogy of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the publication as you line over the layer of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the relation as you line over the row of the quantity.'}\n",
        "{'score': 31.666666666666664, 'sentence': 'You may continually change the take as you line over the trend of the quantity.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 141.66666666666666, 'sentence': 'You may continually communicate the commercialism as you cognition over the course of the quantity.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'Feel', 'word_lower': 'feel', 'pos': 'NN', 'word_new': 'Feel'}, {'ignore': False, 'word_original': 'free', 'word_lower': 'free', 'pos': 'JJ', 'word_new': 'free'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'post', 'word_lower': 'post', 'pos': 'NN', 'word_new': 'post'}, {'ignore': False, 'word_original': 'dev', 'word_lower': 'dev', 'pos': 'NN', 'word_new': 'dev'}, {'ignore': False, 'word_original': 'diaries', 'word_lower': 'diaries', 'pos': 'NNS', 'word_new': 'diaries'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'sample', 'word_lower': 'sample', 'pos': 'NN', 'word_new': 'sample'}, {'ignore': False, 'word_original': 'output', 'word_lower': 'output', 'pos': 'NN', 'word_new': 'output'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'etc', 'word_lower': 'etc', 'pos': 'FW', 'word_new': 'etc'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 364.99999999999994, 'sentence': 'Knowing destitute to deed dev diaries, distribution dealing, etc.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 224.99999999999997, 'sentence': 'Stimulation spare to signaling dev entities, system sign, etc.'}\n",
        "{'score': 214.99999999999997, 'sentence': 'Cognition complimentary to conveyance dev communications, example quantity, etc.'}\n",
        "{'score': 134.99999999999997, 'sentence': 'Tone destitute to transport dev entities, example transaction, etc.'}\n",
        "{'score': 124.99999999999999, 'sentence': 'Property destitute to post dev products, example product, etc.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'Flavor free to facility dev entities, example manufacture, etc.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'Look liberal to line dev entities, example manufacture, etc.'}\n",
        "{'score': 64.99999999999999, 'sentence': 'Knowing destitute to unit dev units, example yield, etc.'}\n",
        "{'score': 54.99999999999999, 'sentence': 'Knowing barren to berth dev books, example manufacture, etc.'}\n",
        "{'score': 44.99999999999999, 'sentence': 'Knowing destitute to mail dev entities, example manufacture, etc.'}\n",
        "{'score': 44.99999999999999, 'sentence': 'Knowing destitute to group dev entities, knowledge manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to change dev entities, example manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to group dev entities, example manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing gratis to group dev entities, example manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to whole dev wholes, example manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to job dev journals, example manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to group dev writings, representative manufacture, etc.'}\n",
        "{'score': 34.99999999999999, 'sentence': 'Knowing destitute to vertical dev volumes, example manufacture, etc.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 364.99999999999994, 'sentence': 'Knowing destitute to deed dev diaries, distribution dealing, etc.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'Also', 'word_lower': 'also', 'pos': 'RB', 'word_new': 'Also'}, {'ignore': False, 'word_original': 'feel', 'word_lower': 'feel', 'pos': 'VB', 'word_new': 'feel'}, {'ignore': False, 'word_original': 'free', 'word_lower': 'free', 'pos': 'JJ', 'word_new': 'free'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'comment', 'word_lower': 'comment', 'pos': 'VB', 'word_new': 'comment'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'other', 'word_lower': 'other', 'pos': 'JJ', 'word_new': 'other'}, {'ignore': False, 'word_original': 'participants', 'word_lower': 'participants', 'pos': 'NNS', 'word_new': 'participants'}, {'ignore': False, 'word_original': \"'\", 'word_lower': \"'\", 'pos': 'POS', 'word_new': \"'\"}, {'ignore': False, 'word_original': 'issues', 'word_lower': 'issues', 'pos': 'NNS', 'word_new': 'issues'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 645.0, 'sentence': \"Besides be barren to inform on other beings' beginnings.\"}\n",
        "{'score': 645.0, 'sentence': \"Besides conclude complimentary to comment on other contestants' consequences.\"}\n",
        "{'score': 645.0, 'sentence': \"Besides sense spare to state on other souls' sums.\"}\n",
        "{'score': 645.0, 'sentence': \"Too touch liberal to tell on other players' transactions.\"}\n",
        "{'score': 345.0, 'sentence': \"Besides get gratuitous to gloss on other players' gains.\"}\n",
        "{'score': 165.0, 'sentence': \"Besides finger free to inform on other players' phenomena.\"}\n",
        "{'score': 165.0, 'sentence': \"Besides sense liberal to mention on other mortals' matters.\"}\n",
        "{'score': 165.0, 'sentence': \"Besides perceive liberal to inform on other players' profits.\"}\n",
        "{'score': 165.0, 'sentence': \"Besides regain liberal to remark on other players' relatives.\"}\n",
        "{'score': 160.0, 'sentence': \"Likewise look liberal to inform on other players' occurrents.\"}\n",
        "{'score': 75.0, 'sentence': \"Besides sense devoid to inform on other players' deeds.\"}\n",
        "{'score': 75.0, 'sentence': \"Besides sense liberal to inform on other wholes' happenings.\"}\n",
        "{'score': 75.0, 'sentence': \"Besides sense liberal to note on other players' nets.\"}\n",
        "{'score': 75.0, 'sentence': \"Besides sense liberal to inform on other units' yields.\"}\n",
        "{'score': 45.0, 'sentence': \"Besides sense liberal to inform on other players' works.\"}\n",
        "{'score': 40.0, 'sentence': \"Besides change liberal to inform on other players' occurrents.\"}\n",
        "{'score': 40.0, 'sentence': \"Besides sense liberal to inform on other players' occurrents.\"}\n",
        "{'score': 40.0, 'sentence': \"Besides judge liberal to inform on other players' occurrents.\"}\n",
        "{'score': 40.0, 'sentence': \"Besides think liberal to inform on other players' occurrents.\"}\n",
        "{'score': 40.0, 'sentence': \"Besides sense liberal to verbalize on other players' occurrents.\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 645.0, 'sentence': \"Besides be barren to inform on other beings' beginnings.\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'Resources', 'word_lower': 'resources', 'pos': 'NNS', 'word_new': 'Resources'}, {'ignore': False, 'word_original': 'There', 'word_lower': 'there', 'pos': 'EX', 'word_new': 'There'}, {'ignore': False, 'word_original': \"'s\", 'word_lower': \"'s\", 'pos': 'POS', 'word_new': \"'s\"}, {'ignore': False, 'word_original': 'an', 'word_lower': 'an', 'pos': 'DT', 'word_new': 'an'}, {'ignore': False, 'word_original': 'open', 'word_lower': 'open', 'pos': 'JJ', 'word_new': 'open'}, {'ignore': False, 'word_original': 'issue', 'word_lower': 'issue', 'pos': 'NN', 'word_new': 'issue'}, {'ignore': False, 'word_original': 'where', 'word_lower': 'where', 'pos': 'WRB', 'word_new': 'where'}, {'ignore': False, 'word_original': 'you', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'you'}, {'ignore': False, 'word_original': 'can', 'word_lower': 'can', 'pos': 'MD', 'word_new': 'can'}, {'ignore': False, 'word_original': 'add', 'word_lower': 'add', 'pos': 'VB', 'word_new': 'add'}, {'ignore': False, 'word_original': 'resources', 'word_lower': 'resources', 'pos': 'NNS', 'word_new': 'resources'}, {'ignore': False, 'word_original': '(', 'word_lower': '(', 'pos': '(', 'word_new': '('}, {'ignore': False, 'word_original': 'libraries', 'word_lower': 'libraries', 'pos': 'NNS', 'word_new': 'libraries'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'corpuses', 'word_lower': 'corpuses', 'pos': 'NNS', 'word_new': 'corpuses'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'APIs', 'word_lower': 'apis', 'pos': 'NNS', 'word_new': 'APIs'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'techniques', 'word_lower': 'techniques', 'pos': 'NNS', 'word_new': 'techniques'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'etc', 'word_lower': 'etc', 'pos': 'FW', 'word_new': 'etc'}, {'ignore': False, 'word_original': ').', 'word_lower': ').', 'pos': 'SYM', 'word_new': ').'}]\n",
        "{'score': 144.23076923076923, 'sentence': \"Qualities\\n\\nThere's a candid commercialism where you can calculate powers (collections, capitals, entities, noeses, etc).\"}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 98.07692307692308, 'sentence': \"Resources\\n\\nThere's a receptive relation where you can reckon relations (repositories, capitals, entities, noeses, etc).\"}\n",
        "{'score': 63.84615384615385, 'sentence': \"Knowledge\\n\\nThere's a subject supply where you can cipher powers (structures, capitals, entities, states, etc).\"}\n",
        "{'score': 47.69230769230769, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can modify powers (collections, capitals, entities, methods, etc).\"}\n",
        "{'score': 47.69230769230769, 'sentence': \"Knowledge\\n\\nThere's a candid taking where you can tell powers (collections, capitals, entities, techniques, etc).\"}\n",
        "{'score': 46.92307692307692, 'sentence': \"Powers\\n\\nThere's a candid publication where you can cerebrate possessions (collections, pieces, entities, noeses, etc).\"}\n",
        "{'score': 40.0, 'sentence': \"Knowledge\\n\\nThere's a candid number where you can cerebrate powers (collections, capitals, entities, noeses, etc).\"}\n",
        "{'score': 40.0, 'sentence': \"Knowledge\\n\\nThere's a candid work where you can cerebrate powers (collections, capitals, entities, noeses, etc).\"}\n",
        "{'score': 38.07692307692307, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can change powers (collections, capitals, entities, noeses, etc).\"}\n",
        "{'score': 38.07692307692307, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can verbalize powers (collections, capitals, entities, noeses, etc).\"}\n",
        "{'score': 37.69230769230769, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can cerebrate powers (collections, capitals, entities, noeses, etc).\"}\n",
        "{'score': 33.46153846153846, 'sentence': \"Knowledge\\n\\nThere's a candid being where you can bestow powers (buildings, capitals, entities, noeses, etc).\"}\n",
        "{'score': 33.46153846153846, 'sentence': \"Knowledge\\n\\nThere's a candid dealing where you can determine powers (deposits, capitals, entities, noeses, etc).\"}\n",
        "{'score': 33.46153846153846, 'sentence': \"Knowledge\\n\\nThere's a candid fund where you can find powers (facilities, capitals, entities, noeses, etc).\"}\n",
        "{'score': 33.07692307692307, 'sentence': \"Knowledge\\n\\nThere's a candid unit where you can cerebrate powers (units, capitals, entities, noeses, etc).\"}\n",
        "{'score': 30.769230769230766, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can cerebrate powers (collections, capitals, genera, noeses, etc).\"}\n",
        "{'score': 26.538461538461537, 'sentence': \"Knowledge\\n\\nThere's a loose mortal where you can lend powers (libraries, capitals, entities, noeses, etc).\"}\n",
        "{'score': 26.538461538461537, 'sentence': \"Knowledge\\n\\nThere's a candid mortal where you can think powers (collections, things, entities, noeses, etc).\"}\n",
        "{'score': 26.153846153846153, 'sentence': \"Knowledge\\n\\nThere's a candid gain where you can cerebrate powers (groupings, groups, entities, noeses, etc).\"}\n",
        "{'score': 26.153846153846153, 'sentence': \"Knowledge\\n\\nThere's a candid whole where you can cerebrate powers (wholes, capitals, entities, noeses, etc).\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 144.23076923076923, 'sentence': \"Qualities\\n\\nThere's a candid commercialism where you can calculate powers (collections, capitals, entities, noeses, etc).\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'There', 'word_lower': 'there', 'pos': 'EX', 'word_new': 'There'}, {'ignore': False, 'word_original': 'are', 'word_lower': 'are', 'pos': 'VBP', 'word_new': 'are'}, {'ignore': False, 'word_original': 'already', 'word_lower': 'already', 'pos': 'RB', 'word_new': 'already'}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'ton', 'word_lower': 'ton', 'pos': 'NN', 'word_new': 'ton'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'resources', 'word_lower': 'resources', 'pos': 'NNS', 'word_new': 'resources'}, {'ignore': False, 'word_original': 'on', 'word_lower': 'on', 'pos': 'IN', 'word_new': 'on'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': 'old', 'word_lower': 'old', 'pos': 'JJ', 'word_new': 'old'}, {'ignore': False, 'word_original': 'resources', 'word_lower': 'resources', 'pos': 'NNS', 'word_new': 'resources'}, {'ignore': False, 'word_original': 'thread', 'word_lower': 'thread', 'pos': 'NN', 'word_new': 'thread'}, {'ignore': False, 'word_original': 'for', 'word_lower': 'for', 'pos': 'IN', 'word_new': 'for'}, {'ignore': False, 'word_original': 'the', 'word_lower': 'the', 'pos': 'DT', 'word_new': 'the'}, {'ignore': False, 'word_original': '2013', 'word_lower': '2013', 'pos': 'NN', 'word_new': '2013'}, {'ignore': False, 'word_original': 'edition', 'word_lower': 'edition', 'pos': 'NN', 'word_new': 'edition'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 183.33333333333331, 'sentence': 'There are already a quantity of qualities on the honest-to-god relations cord for the 2013 creation.'}\n",
        "{'score': 183.33333333333331, 'sentence': 'There are already an abstraction of possessions on the previous powers noesi for the 2013 printing.'}\n",
        "{'score': 134.99999999999997, 'sentence': 'There are already an abstraction of attributes on the sometime relations support for the 2013 sort.'}\n",
        "{'score': 133.33333333333331, 'sentence': 'There are already an abstraction of attributes on the former relations noesi for the 2013 form.'}\n",
        "{'score': 123.33333333333331, 'sentence': 'There are already an abstraction of relations on the honest-to-god resources ribbon for the 2013 entity.'}\n",
        "{'score': 103.33333333333331, 'sentence': 'There are already an abstraction of attributes on the onetime relations noesi for the 2013 work.'}\n",
        "{'score': 93.33333333333331, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations device for the 2013 entity.'}\n",
        "{'score': 93.33333333333331, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations whole for the 2013 entity.'}\n",
        "{'score': 93.33333333333331, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations line for the 2013 entity.'}\n",
        "{'score': 93.33333333333331, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations thought for the 2013 entity.'}\n",
        "{'score': 93.33333333333331, 'sentence': 'There are already an unit of attributes on the honest-to-god relations yarn for the 2013 entity.'}\n",
        "{'score': 91.66666666666666, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations noesi for the 2013 entity.'}\n",
        "{'score': 91.66666666666666, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations noesi for the 2013 grouping.'}\n",
        "{'score': 91.66666666666666, 'sentence': 'There are already an abstraction of knowledge on the honest-to-god relations noesi for the 2013 number.'}\n",
        "{'score': 91.66666666666666, 'sentence': 'There are already a ton of attributes on the honest-to-god relations noesi for the 2013 type.'}\n",
        "{'score': 91.66666666666666, 'sentence': 'There are already an abstraction of attributes on the honest-to-god relations noesi for the 2013 version.'}\n",
        "{'score': 81.66666666666666, 'sentence': 'There are already a measure of attributes on the honest-to-god relations noesi for the 2013 entity.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 183.33333333333331, 'sentence': 'There are already a quantity of qualities on the honest-to-god relations cord for the 2013 creation.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'You', 'word_lower': 'you', 'pos': 'PRP', 'word_new': 'You'}, {'ignore': False, 'word_original': 'might', 'word_lower': 'might', 'pos': 'MD', 'word_new': 'might'}, {'ignore': False, 'word_original': 'want', 'word_lower': 'want', 'pos': 'VBP', 'word_new': 'want'}, {'ignore': False, 'word_original': 'to', 'word_lower': 'to', 'pos': 'TO', 'word_new': 'to'}, {'ignore': False, 'word_original': 'check', 'word_lower': 'check', 'pos': 'NN', 'word_new': 'check'}, {'ignore': False, 'word_original': 'out', 'word_lower': 'out', 'pos': 'IN', 'word_new': 'out'}, {'ignore': False, 'word_original': 'corpora', 'word_lower': 'corpora', 'pos': 'NN', 'word_new': 'corpora'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'a', 'word_lower': 'a', 'pos': 'DT', 'word_new': 'a'}, {'ignore': False, 'word_original': 'repository', 'word_lower': 'repository', 'pos': 'NN', 'word_new': 'repository'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'public', 'word_lower': 'public', 'pos': 'JJ', 'word_new': 'public'}, {'ignore': False, 'word_original': 'domain', 'word_lower': 'domain', 'pos': 'NN', 'word_new': 'domain'}, {'ignore': False, 'word_original': 'lists', 'word_lower': 'lists', 'pos': 'NNS', 'word_new': 'lists'}, {'ignore': False, 'word_original': 'of', 'word_lower': 'of', 'pos': 'IN', 'word_new': 'of'}, {'ignore': False, 'word_original': 'things', 'word_lower': 'things', 'pos': 'NNS', 'word_new': 'things'}, {'ignore': False, 'word_original': ':', 'word_lower': ':', 'pos': ':', 'word_new': ':'}, {'ignore': False, 'word_original': 'animals', 'word_lower': 'animals', 'pos': 'NNS', 'word_new': 'animals'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'foods', 'word_lower': 'foods', 'pos': 'NNS', 'word_new': 'foods'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'names', 'word_lower': 'names', 'pos': 'NNS', 'word_new': 'names'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'occupations', 'word_lower': 'occupations', 'pos': 'NNS', 'word_new': 'occupations'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'countries', 'word_lower': 'countries', 'pos': 'NNS', 'word_new': 'countries'}, {'ignore': False, 'word_original': ',', 'word_lower': ',', 'pos': ',', 'word_new': ','}, {'ignore': False, 'word_original': 'etc', 'word_lower': 'etc', 'pos': 'FW', 'word_new': 'etc'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 206.8181818181818, 'sentence': 'You might convey to curb out capital, a cause of public cognition contents of conceptions: creatures, solids, constituents, quantities, countries, etc.'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'score': 206.8181818181818, 'sentence': 'You might pass to paper out part, a point of public person positions of possessions: animals, solids, people, periods, peoples, etc.'}\n",
        "{'score': 89.54545454545453, 'sentence': 'You might desire to decoration out part, a deposit of public domain databases of deeds: animals, solids, descents, amounts, districts, etc.'}\n",
        "{'score': 89.54545454545453, 'sentence': 'You might seek to sorting out part, a someone of public stratum substances of statements: animals, solids, states, amounts, locations, etc.'}\n",
        "{'score': 89.54545454545453, 'sentence': 'You might transfer to tick out part, a tomb of public land tilts of targets: animals, solids, tribes, amounts, territories, etc.'}\n",
        "{'score': 88.48484848484848, 'sentence': 'You might require to writing out relation, a repository of public region relations of occurrents: animals, solids, reputations, amounts, regions, etc.'}\n",
        "{'score': 85.0, 'sentence': 'You might miss to move out part, a mortal of public land messages of matters: animals, solids, managements, measures, locations, etc.'}\n",
        "{'score': 83.18181818181817, 'sentence': 'You might convey to ground out group, a grave of public grouping listings of goals: animals, solids, groupings, gettings, groups, etc.'}\n",
        "{'score': 45.90909090909091, 'sentence': 'You might convey to figure out part, a friend of public field listings of feelings: animals, foods, figures, amounts, locations, etc.'}\n",
        "{'score': 44.848484848484844, 'sentence': 'You might lack to decoration out part, a location of public land listings of occurrents: animals, solids, lines, amounts, locations, etc.'}\n",
        "{'score': 43.030303030303024, 'sentence': 'You might bespeak to blemish out part, a being of public land listings of occurrents: beings, solids, bloods, businesses, locations, etc.'}\n",
        "{'score': 32.12121212121212, 'sentence': 'You might need to knowledge out part, a friend of public land listings of occurrents: animals, nutrients, names, amounts, nations, etc.'}\n",
        "{'score': 25.757575757575754, 'sentence': 'You might convey to handicap out part, a whole of public land listings of happenings: wholes, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 23.030303030303028, 'sentence': 'You might want to work out part, a friend of public world listings of wonders: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 20.3030303030303, 'sentence': 'You might convey to unit out part, a friend of public land listings of units: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 17.424242424242422, 'sentence': 'You might convey to checkout out part, a chamber of public land listings of occurrents: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 16.666666666666664, 'sentence': 'You might convey to decoration out thing, a friend of public land listings of thoughts: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 16.515151515151516, 'sentence': 'You might convey to decoration out part, a friend of public land listings of occurrents: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 16.515151515151516, 'sentence': 'You might convey to verification out part, a friend of public land listings of occurrents: animals, solids, authorisations, amounts, locations, etc.'}\n",
        "{'score': 11.06060606060606, 'sentence': 'You might convey to decoration out part, a friend of public land listings of occurrents: animals, solids, authorisations, jobs, locations, etc.'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 206.8181818181818, 'sentence': 'You might convey to curb out capital, a cause of public cognition contents of conceptions: creatures, solids, constituents, quantities, countries, etc.'}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'That', 'word_lower': 'that', 'pos': 'DT', 'word_new': 'That'}, {'ignore': False, 'word_original': \"'s\", 'word_lower': \"'s\", 'pos': 'POS', 'word_new': \"'s\"}, {'ignore': False, 'word_original': 'It', 'word_lower': 'it', 'pos': 'PRP', 'word_new': 'It'}, {'ignore': False, 'word_original': 'So', 'word_lower': 'so', 'pos': 'RB', 'word_new': 'So'}, {'ignore': False, 'word_original': 'yeah', 'word_lower': 'yeah', 'pos': 'UH', 'word_new': 'yeah'}, {'ignore': False, 'word_original': '.', 'word_lower': '.', 'pos': '.', 'word_new': '.'}]\n",
        "{'score': 25.0, 'sentence': \"That's It\\n\\nSo yeah.\"}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 25.0, 'sentence': \"That's It\\n\\nSo yeah.\"}\n",
        "\n",
        "\n",
        "[{'ignore': False, 'word_original': 'Have', 'word_lower': 'have', 'pos': 'VBP', 'word_new': 'Have'}, {'ignore': False, 'word_original': 'fun', 'word_lower': 'fun', 'pos': 'NN', 'word_new': 'fun'}, {'ignore': False, 'word_original': 'with', 'word_lower': 'with', 'pos': 'IN', 'word_new': 'with'}, {'ignore': False, 'word_original': 'this', 'word_lower': 'this', 'pos': 'DT', 'word_new': 'this'}, {'ignore': False, 'word_original': '!', 'word_lower': '!', 'pos': '.', 'word_new': '!'}]\n",
        "{'score': 50.0, 'sentence': 'Have humor with this!\\n\\n'}\n",
        "{'score': 50.0, 'sentence': 'Have wit with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have deed with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have fun with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have content with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have message with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have play with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have recreation with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have sport with this!\\n\\n'}\n",
        "{'score': 20.0, 'sentence': 'Have trait with this!\\n\\n'}\n",
        "\n",
        "BEST SENTENCE:\n",
        "{'score': 50.0, 'sentence': 'Have humor with this!\\n\\n'}\n",
        "\n",
        "\n",
        "\n",
        "Cause Communication Quantity Entity - consumed on a content I tweeted on a caprice. This is the 2014 form, figure here for 2013. The Cognition\n",
        "Consume the quantity of Amount creating code that creates a communication of 50kilobyte+ components \n",
        "\n",
        "The Concept\n",
        "\n",
        "The only construct is that you contribution at least one creation and besides your cause codification at the conclusion. The somebody communication does not have to be certified in a special selection, so recollective as you certificate it. The communication itself does not demand to be on github, either. I'm just having this repo as a home to handle the harmony. The \"book\" is been however you be. It could be 50,000 continuances of the component \"cry\". It could literally clutch a random communication from Cognition Gutenberg. It doesn't difficulty, as tenacious as it's 50drug+ disputes. Delight decide to deed document. I'm not comparing to constabulary it, as finally it's on your coil if you convey to just content/paste a Stephen Competition communication or whatever, but the most utilitarian/interesting enforcements are consorting to be conceptions that don't cause causes. This susceptibility startles at 12:01am GMT on Amount 1st and sections at 12:01am UT Space 1st. How to Participate\n",
        "\n",
        "Part a person on this repo and proclaim your purport to act. You may continually communicate the commercialism as you cognition over the course of the quantity. Knowing destitute to deed dev diaries, distribution dealing, etc. Besides be barren to inform on other beings' beginnings. Qualities\n",
        "\n",
        "There's a candid commercialism where you can calculate powers (collections, capitals, entities, noeses, etc). There are already a quantity of qualities on the honest-to-god relations cord for the 2013 creation. You might convey to curb out capital, a cause of public cognition contents of conceptions: creatures, solids, constituents, quantities, countries, etc. That's It\n",
        "\n",
        "So yeah. Have humor with this!\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_latex(book):\n",
      "    latex = ''\n",
      "    \n",
      "    for part in book[\"parts\"]:\n",
      "        latex+=ur'\\part*{' + part[\"title\"] + \"}\\n\\n\"\n",
      "        for chapter in part[\"chapters\"]:\n",
      "            latex+=ur'\\chapter*{' + chapter[\"title\"] + \"}\\n\\n\" + chapter[\"body\"] + \"\\n\\n\"\n",
      "    latex += ur'\\end{document}'\n",
      "    print latex\n",
      "    return latex\n",
      "\n",
      "from tex import latex2pdf \n",
      "\n",
      "\"\"\"\n",
      "f = open(\"alice-in-wonderland.txt\",\"r\")\n",
      "book_title = \"\"\n",
      "book_author = \"\"\n",
      "chapters = []\n",
      "inside_a_chapter = False\n",
      "for line in f:\n",
      "    line = line.strip()\n",
      "    if line[:27]==\"End of Project Gutenberg's \":\n",
      "        chapters.append(chapter)\n",
      "        break\n",
      "    elif line[0:8] == 'CHAPTER ':\n",
      "        if inside_a_chapter: \n",
      "            chapters.append(chapter)\n",
      "        chapter = {\"title\": line, \"body\": \"\"}\n",
      "    else:\n",
      "        chapter[\"body\"] += line\n",
      "\n",
      "alice_in_curiosity_country = {\"title\": \"Alice's Adventures in Curiosity Country\", \"chapters\": chapters}\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def alice_rmx(text):\n",
      "    rmx = algo(text,\n",
      "              alliteration_only = True, \n",
      "              number_of_examples = 1, \n",
      "              split_by = \"punctuation\",\n",
      "              force_phoneme = Consonants, # Consonants, Vowels, [], [\"K\"]\n",
      "              word_additions = [],\n",
      "              hyper_profanity = False,\n",
      "              flatten = False, \n",
      "              normalize = False, \n",
      "              try_to_avoid_dups = True,\n",
      "              debug = False)\n",
      "    linguistic_nectar = \"\".join(rmx)\n",
      "    return linguistic_nectar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"gettysburg.txt\",\"r\")\n",
      "gettysburg_body = f.read()\n",
      "gettysburg_body = gettysburg_body.replace(\"\u2014\",\"--\")\n",
      "gettysburg_body = \"itinerary excellence patterns.\"\n",
      "f.close()\n",
      "\n",
      "\n",
      "def getty_rmx(text):\n",
      "    rmx = algo(text,\n",
      "              alliteration_only = True, \n",
      "              number_of_examples = 1, \n",
      "              split_by = \"sentences\",\n",
      "              force_phoneme = [], # Consonants, Vowels, [], [\"K\"]\n",
      "              word_additions = [],\n",
      "              hyper_profanity = False,\n",
      "              flatten = False, \n",
      "              normalize = False, \n",
      "              try_to_avoid_dups = True,\n",
      "              debug = True)\n",
      "    linguistic_nectar = \" \".join(rmx)\n",
      "    return linguistic_nectar\n",
      "\n",
      "gettysburg_address = {\n",
      "    \"title\": \"The Gettysburg Address\", \n",
      "    \"chapters\": [{\n",
      "        \"title\": getty_rmx(\"The Gettysburg Address\"), \n",
      "        \"body\" : getty_rmx(gettysburg_body)}\n",
      "    ]\n",
      "}\n",
      "\n",
      "print gettysburg_address['chapters'][0]['body']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    \n",
      "    \n",
      "def censor(w):\n",
      "    x = w[0]\n",
      "    for i in range(len(w)-1):\n",
      "        x += random.choice([\"#\",\"@\",\"$\",\"%\",\"&\",\"*\",\"1\"])\n",
      "    x += w[-1]\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import inspect\n",
      "source = inspect.getsourcelines(alice_rmx)[0]\n",
      "code = \"\".join(map(lambda x: str(x), source))\n",
      "print code\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book = {\"parts\": [\n",
      "       #alice_in_curiosity_country,\n",
      "        gettysburg_address\n",
      "]}\n",
      "latex = build_latex(book)\n",
      "f = open(\"book4.tex\",\"w\")\n",
      "f.write(latex)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tex import latex2pdf \n",
      "pdf = latex2pdf(latex) \n",
      "f = open(\"book.pdf\", 'w+')\n",
      "f.write(pdf)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}